program: train_model.py
name: DeiT_tiny_efficientnet_hyperparameter_tunning_weighted_batch
method: random
metric:
  goal: minimize
  name: Val mean loss per epoch
parameters:
  daug_horizontalflip:
    values:
      - True
      - False
    distribution: categorical
  daug_shiftscalerotate:
    values:
      - True
      - False
    distribution: categorical
  daug_coarsedropout:
    values:
      - True
      - False
    distribution: categorical
  daug_colorjitter:
    values:
      - True
      - False
    distribution: categorical
  daug_gaussnoise:
    values:
      - True
      - False
    distribution: categorical
  daug_p_value:
    max: 0.75
    min: 0.25
    distribution: uniform
  image_norm:
    values:
      - "imagenet"
      - "affectnet"
    distribution: categorical
  arch:
    values:
      - "deit_tiny"
  lr:
    values: [0.0000075, 0.000005, 0.0000025, 0.000001, 0.00000075, 0.0000005, 0.00000025, 0.0000001]
    distribution: categorical
  batch_size:
    values: [64, 96, 128, 160, 192]
    distribution: categorical
  optimizer:
    values: 
      - "adamw"
    distribution: categorical
  momentum:
    values:
      - 'none'
  pretraining:
    values:
     - "imagenet"
  label_smoothing:
    max: 0.1
    min: 0
    distribution: uniform
  epochs:
    values: 
      - 30
  weighted_sampler_train: 
    values:
      - True
  weighted_sampler_val: 
    values:
      - True
  weighted_loss:
    values:
      - False
  patience:
    values: 
      - 5
  epoch_samples:
    values: 
      - "original"
  random_seed:
    values:
      - 33
  distillation:
    values: 
      - True
  alpha:
    max: 0.65
    min: 0.35
    distribution: uniform
  decaying_strategy: 
    values:
      - "0"
      - "1"
      - "2"
      - "3"
  embedding_method: 
    values:
      - "class"
  teacher_arch: 
    values:
      - "efficientnet_b0"
    