{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "# To add src to the path so that we can import modules\n",
    "current_directory = os.getcwd()\n",
    "if not current_directory.endswith(\"emotion_recognition\"):\n",
    "    sys.path.append(os.path.join(current_directory, 'emotion_recognition'))\n",
    "\n",
    "try:\n",
    "    from src import NUMBER_OF_EMOT, MODELS_DIR\n",
    "    import src.models.architectures as arch\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Ensure that src is added to PATH and restart the kernel\")\n",
    "    print(sys.path)\n",
    "\n",
    "# Take GPU\n",
    "if not torch.cuda.is_available():\n",
    "       raise RuntimeError(\"Enable GPU support\")\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/usuaris/imatge/armand.de.asis/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 158MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.56\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 384.62\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(weights = \"DEFAULT\").to(device)\n",
    "print(summary(model, (3, 224, 224))) # Summary of the model with input size (3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try changing last FC layer in order to adapt to our task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(2048, NUMBER_OF_EMOT)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size is compatible with layer sizes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model(torch.rand((1, 3, 224, 224)).to(device))\n",
    "    print(\"Image size is compatible with layer sizes.\")\n",
    "except RuntimeError as e:\n",
    "    e = str(e)\n",
    "    if e.endswith(\"Output size is too small\"):\n",
    "        print(\"Image size is too small.\")\n",
    "    elif \"shapes cannot be multiplied\" in e:\n",
    "        required_shape = e[e.index(\"x\") + 1:].split(\" \")[0]\n",
    "        print(f\"Linear layer needs to have size: {required_shape}\")\n",
    "    else:\n",
    "        print(f\"Error not understood: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try hooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple hook class that returns the input and output of a layer during forward/backward pass\n",
    "class Hook():\n",
    "    def __init__(self, module):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.output = output\n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 8])\n",
      "torch.Size([48, 2048, 7, 7])\n",
      "torch.Size([48, 8])\n",
      "torch.Size([48, 2048, 7, 7])\n",
      "torch.Size([48, 8])\n",
      "torch.Size([48, 2048, 7, 7])\n",
      "torch.Size([48, 8])\n",
      "torch.Size([48, 2048, 7, 7])\n",
      "torch.Size([48, 8])\n",
      "torch.Size([48, 2048, 7, 7])\n",
      "torch.Size([48, 8])\n",
      "torch.Size([48, 2048, 7, 7])\n",
      "torch.Size([48, 8])\n",
      "torch.Size([48, 2048, 7, 7])\n",
      "torch.Size([48, 8])\n",
      "torch.Size([48, 2048, 7, 7])\n",
      "torch.Size([48, 8])\n",
      "torch.Size([48, 2048, 7, 7])\n",
      "torch.Size([48, 8])\n",
      "torch.Size([48, 2048, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    # Register the hook for the last layer of layer4 (Bottleneck-172)\n",
    "    handle = Hook(model.layer4[-1])\n",
    "    # Run the model\n",
    "    input_tensor = torch.rand(48, 3, 224, 224).to(device)  # Batch size 32\n",
    "\n",
    "    output = model(input_tensor)\n",
    "    print(output.size())\n",
    "    print(handle.output.size())\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNext50_32x4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth\" to /home/usuaris/imatge/armand.de.asis/.cache/torch/hub/checkpoints/resnext50_32x4d-1a0047aa.pth\n",
      "100%|██████████| 95.8M/95.8M [00:00<00:00, 159MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5          [-1, 128, 56, 56]           8,192\n",
      "       BatchNorm2d-6          [-1, 128, 56, 56]             256\n",
      "              ReLU-7          [-1, 128, 56, 56]               0\n",
      "            Conv2d-8          [-1, 128, 56, 56]           4,608\n",
      "       BatchNorm2d-9          [-1, 128, 56, 56]             256\n",
      "             ReLU-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          32,768\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-18          [-1, 128, 56, 56]             256\n",
      "             ReLU-19          [-1, 128, 56, 56]               0\n",
      "           Conv2d-20          [-1, 128, 56, 56]           4,608\n",
      "      BatchNorm2d-21          [-1, 128, 56, 56]             256\n",
      "             ReLU-22          [-1, 128, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          32,768\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-28          [-1, 128, 56, 56]             256\n",
      "             ReLU-29          [-1, 128, 56, 56]               0\n",
      "           Conv2d-30          [-1, 128, 56, 56]           4,608\n",
      "      BatchNorm2d-31          [-1, 128, 56, 56]             256\n",
      "             ReLU-32          [-1, 128, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          32,768\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 256, 56, 56]          65,536\n",
      "      BatchNorm2d-38          [-1, 256, 56, 56]             512\n",
      "             ReLU-39          [-1, 256, 56, 56]               0\n",
      "           Conv2d-40          [-1, 256, 28, 28]          18,432\n",
      "      BatchNorm2d-41          [-1, 256, 28, 28]             512\n",
      "             ReLU-42          [-1, 256, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-50          [-1, 256, 28, 28]             512\n",
      "             ReLU-51          [-1, 256, 28, 28]               0\n",
      "           Conv2d-52          [-1, 256, 28, 28]          18,432\n",
      "      BatchNorm2d-53          [-1, 256, 28, 28]             512\n",
      "             ReLU-54          [-1, 256, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-60          [-1, 256, 28, 28]             512\n",
      "             ReLU-61          [-1, 256, 28, 28]               0\n",
      "           Conv2d-62          [-1, 256, 28, 28]          18,432\n",
      "      BatchNorm2d-63          [-1, 256, 28, 28]             512\n",
      "             ReLU-64          [-1, 256, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-70          [-1, 256, 28, 28]             512\n",
      "             ReLU-71          [-1, 256, 28, 28]               0\n",
      "           Conv2d-72          [-1, 256, 28, 28]          18,432\n",
      "      BatchNorm2d-73          [-1, 256, 28, 28]             512\n",
      "             ReLU-74          [-1, 256, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 512, 28, 28]         262,144\n",
      "      BatchNorm2d-80          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-81          [-1, 512, 28, 28]               0\n",
      "           Conv2d-82          [-1, 512, 14, 14]          73,728\n",
      "      BatchNorm2d-83          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-84          [-1, 512, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 512, 14, 14]         524,288\n",
      "      BatchNorm2d-92          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-93          [-1, 512, 14, 14]               0\n",
      "           Conv2d-94          [-1, 512, 14, 14]          73,728\n",
      "      BatchNorm2d-95          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-96          [-1, 512, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-102          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-103          [-1, 512, 14, 14]               0\n",
      "          Conv2d-104          [-1, 512, 14, 14]          73,728\n",
      "     BatchNorm2d-105          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-106          [-1, 512, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-112          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-113          [-1, 512, 14, 14]               0\n",
      "          Conv2d-114          [-1, 512, 14, 14]          73,728\n",
      "     BatchNorm2d-115          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-116          [-1, 512, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-122          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-123          [-1, 512, 14, 14]               0\n",
      "          Conv2d-124          [-1, 512, 14, 14]          73,728\n",
      "     BatchNorm2d-125          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-126          [-1, 512, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-132          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-133          [-1, 512, 14, 14]               0\n",
      "          Conv2d-134          [-1, 512, 14, 14]          73,728\n",
      "     BatchNorm2d-135          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-136          [-1, 512, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141         [-1, 1024, 14, 14]       1,048,576\n",
      "     BatchNorm2d-142         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-143         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-144           [-1, 1024, 7, 7]         294,912\n",
      "     BatchNorm2d-145           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-146           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153           [-1, 1024, 7, 7]       2,097,152\n",
      "     BatchNorm2d-154           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-155           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-156           [-1, 1024, 7, 7]         294,912\n",
      "     BatchNorm2d-157           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-158           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163           [-1, 1024, 7, 7]       2,097,152\n",
      "     BatchNorm2d-164           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-165           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-166           [-1, 1024, 7, 7]         294,912\n",
      "     BatchNorm2d-167           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-168           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,028,904\n",
      "Trainable params: 25,028,904\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 361.78\n",
      "Params size (MB): 95.48\n",
      "Estimated Total Size (MB): 457.83\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = models.resnext50_32x4d(weights = \"DEFAULT\").to(device)\n",
    "print(summary(model, (3, 224, 224))) # Summary of the model with input size (3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(2048, NUMBER_OF_EMOT)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size is compatible with layer sizes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model(torch.rand((1, 3, 224, 224)).to(device))\n",
    "    print(\"Image size is compatible with layer sizes.\")\n",
    "except RuntimeError as e:\n",
    "    e = str(e)\n",
    "    if e.endswith(\"Output size is too small\"):\n",
    "        print(\"Image size is too small.\")\n",
    "    elif \"shapes cannot be multiplied\" in e:\n",
    "        required_shape = e[e.index(\"x\") + 1:].split(\" \")[0]\n",
    "        print(f\"Linear layer needs to have size: {required_shape}\")\n",
    "    else:\n",
    "        print(f\"Error not understood: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet B2 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('tf_efficientnet_b2', pretrained=False)\n",
    "model.classifier=nn.Sequential(nn.Linear(in_features=1408, out_features=NUMBER_OF_EMOT)) #1792 #1280 #1536\n",
    "model.to(device)\n",
    "print(model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "        Conv2dSame-1         [-1, 32, 112, 112]             864\n",
      "          Identity-2         [-1, 32, 112, 112]               0\n",
      "              SiLU-3         [-1, 32, 112, 112]               0\n",
      "    BatchNormAct2d-4         [-1, 32, 112, 112]              64\n",
      "            Conv2d-5         [-1, 32, 112, 112]             288\n",
      "          Identity-6         [-1, 32, 112, 112]               0\n",
      "              SiLU-7         [-1, 32, 112, 112]               0\n",
      "    BatchNormAct2d-8         [-1, 32, 112, 112]              64\n",
      "            Conv2d-9              [-1, 8, 1, 1]             264\n",
      "             SiLU-10              [-1, 8, 1, 1]               0\n",
      "           Conv2d-11             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-12             [-1, 32, 1, 1]               0\n",
      "    SqueezeExcite-13         [-1, 32, 112, 112]               0\n",
      "           Conv2d-14         [-1, 16, 112, 112]             512\n",
      "         Identity-15         [-1, 16, 112, 112]               0\n",
      "         Identity-16         [-1, 16, 112, 112]               0\n",
      "   BatchNormAct2d-17         [-1, 16, 112, 112]              32\n",
      "DepthwiseSeparableConv-18         [-1, 16, 112, 112]               0\n",
      "           Conv2d-19         [-1, 16, 112, 112]             144\n",
      "         Identity-20         [-1, 16, 112, 112]               0\n",
      "             SiLU-21         [-1, 16, 112, 112]               0\n",
      "   BatchNormAct2d-22         [-1, 16, 112, 112]              32\n",
      "           Conv2d-23              [-1, 4, 1, 1]              68\n",
      "             SiLU-24              [-1, 4, 1, 1]               0\n",
      "           Conv2d-25             [-1, 16, 1, 1]              80\n",
      "          Sigmoid-26             [-1, 16, 1, 1]               0\n",
      "    SqueezeExcite-27         [-1, 16, 112, 112]               0\n",
      "           Conv2d-28         [-1, 16, 112, 112]             256\n",
      "         Identity-29         [-1, 16, 112, 112]               0\n",
      "         Identity-30         [-1, 16, 112, 112]               0\n",
      "   BatchNormAct2d-31         [-1, 16, 112, 112]              32\n",
      "         Identity-32         [-1, 16, 112, 112]               0\n",
      "DepthwiseSeparableConv-33         [-1, 16, 112, 112]               0\n",
      "           Conv2d-34         [-1, 96, 112, 112]           1,536\n",
      "         Identity-35         [-1, 96, 112, 112]               0\n",
      "             SiLU-36         [-1, 96, 112, 112]               0\n",
      "   BatchNormAct2d-37         [-1, 96, 112, 112]             192\n",
      "       Conv2dSame-38           [-1, 96, 56, 56]             864\n",
      "         Identity-39           [-1, 96, 56, 56]               0\n",
      "             SiLU-40           [-1, 96, 56, 56]               0\n",
      "   BatchNormAct2d-41           [-1, 96, 56, 56]             192\n",
      "           Conv2d-42              [-1, 4, 1, 1]             388\n",
      "             SiLU-43              [-1, 4, 1, 1]               0\n",
      "           Conv2d-44             [-1, 96, 1, 1]             480\n",
      "          Sigmoid-45             [-1, 96, 1, 1]               0\n",
      "    SqueezeExcite-46           [-1, 96, 56, 56]               0\n",
      "           Conv2d-47           [-1, 24, 56, 56]           2,304\n",
      "         Identity-48           [-1, 24, 56, 56]               0\n",
      "         Identity-49           [-1, 24, 56, 56]               0\n",
      "   BatchNormAct2d-50           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-51           [-1, 24, 56, 56]               0\n",
      "           Conv2d-52          [-1, 144, 56, 56]           3,456\n",
      "         Identity-53          [-1, 144, 56, 56]               0\n",
      "             SiLU-54          [-1, 144, 56, 56]               0\n",
      "   BatchNormAct2d-55          [-1, 144, 56, 56]             288\n",
      "           Conv2d-56          [-1, 144, 56, 56]           1,296\n",
      "         Identity-57          [-1, 144, 56, 56]               0\n",
      "             SiLU-58          [-1, 144, 56, 56]               0\n",
      "   BatchNormAct2d-59          [-1, 144, 56, 56]             288\n",
      "           Conv2d-60              [-1, 6, 1, 1]             870\n",
      "             SiLU-61              [-1, 6, 1, 1]               0\n",
      "           Conv2d-62            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-63            [-1, 144, 1, 1]               0\n",
      "    SqueezeExcite-64          [-1, 144, 56, 56]               0\n",
      "           Conv2d-65           [-1, 24, 56, 56]           3,456\n",
      "         Identity-66           [-1, 24, 56, 56]               0\n",
      "         Identity-67           [-1, 24, 56, 56]               0\n",
      "   BatchNormAct2d-68           [-1, 24, 56, 56]              48\n",
      "         Identity-69           [-1, 24, 56, 56]               0\n",
      " InvertedResidual-70           [-1, 24, 56, 56]               0\n",
      "           Conv2d-71          [-1, 144, 56, 56]           3,456\n",
      "         Identity-72          [-1, 144, 56, 56]               0\n",
      "             SiLU-73          [-1, 144, 56, 56]               0\n",
      "   BatchNormAct2d-74          [-1, 144, 56, 56]             288\n",
      "           Conv2d-75          [-1, 144, 56, 56]           1,296\n",
      "         Identity-76          [-1, 144, 56, 56]               0\n",
      "             SiLU-77          [-1, 144, 56, 56]               0\n",
      "   BatchNormAct2d-78          [-1, 144, 56, 56]             288\n",
      "           Conv2d-79              [-1, 6, 1, 1]             870\n",
      "             SiLU-80              [-1, 6, 1, 1]               0\n",
      "           Conv2d-81            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-82            [-1, 144, 1, 1]               0\n",
      "    SqueezeExcite-83          [-1, 144, 56, 56]               0\n",
      "           Conv2d-84           [-1, 24, 56, 56]           3,456\n",
      "         Identity-85           [-1, 24, 56, 56]               0\n",
      "         Identity-86           [-1, 24, 56, 56]               0\n",
      "   BatchNormAct2d-87           [-1, 24, 56, 56]              48\n",
      "         Identity-88           [-1, 24, 56, 56]               0\n",
      " InvertedResidual-89           [-1, 24, 56, 56]               0\n",
      "           Conv2d-90          [-1, 144, 56, 56]           3,456\n",
      "         Identity-91          [-1, 144, 56, 56]               0\n",
      "             SiLU-92          [-1, 144, 56, 56]               0\n",
      "   BatchNormAct2d-93          [-1, 144, 56, 56]             288\n",
      "       Conv2dSame-94          [-1, 144, 28, 28]           3,600\n",
      "         Identity-95          [-1, 144, 28, 28]               0\n",
      "             SiLU-96          [-1, 144, 28, 28]               0\n",
      "   BatchNormAct2d-97          [-1, 144, 28, 28]             288\n",
      "           Conv2d-98              [-1, 6, 1, 1]             870\n",
      "             SiLU-99              [-1, 6, 1, 1]               0\n",
      "          Conv2d-100            [-1, 144, 1, 1]           1,008\n",
      "         Sigmoid-101            [-1, 144, 1, 1]               0\n",
      "   SqueezeExcite-102          [-1, 144, 28, 28]               0\n",
      "          Conv2d-103           [-1, 48, 28, 28]           6,912\n",
      "        Identity-104           [-1, 48, 28, 28]               0\n",
      "        Identity-105           [-1, 48, 28, 28]               0\n",
      "  BatchNormAct2d-106           [-1, 48, 28, 28]              96\n",
      "InvertedResidual-107           [-1, 48, 28, 28]               0\n",
      "          Conv2d-108          [-1, 288, 28, 28]          13,824\n",
      "        Identity-109          [-1, 288, 28, 28]               0\n",
      "            SiLU-110          [-1, 288, 28, 28]               0\n",
      "  BatchNormAct2d-111          [-1, 288, 28, 28]             576\n",
      "          Conv2d-112          [-1, 288, 28, 28]           7,200\n",
      "        Identity-113          [-1, 288, 28, 28]               0\n",
      "            SiLU-114          [-1, 288, 28, 28]               0\n",
      "  BatchNormAct2d-115          [-1, 288, 28, 28]             576\n",
      "          Conv2d-116             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-117             [-1, 12, 1, 1]               0\n",
      "          Conv2d-118            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-119            [-1, 288, 1, 1]               0\n",
      "   SqueezeExcite-120          [-1, 288, 28, 28]               0\n",
      "          Conv2d-121           [-1, 48, 28, 28]          13,824\n",
      "        Identity-122           [-1, 48, 28, 28]               0\n",
      "        Identity-123           [-1, 48, 28, 28]               0\n",
      "  BatchNormAct2d-124           [-1, 48, 28, 28]              96\n",
      "        Identity-125           [-1, 48, 28, 28]               0\n",
      "InvertedResidual-126           [-1, 48, 28, 28]               0\n",
      "          Conv2d-127          [-1, 288, 28, 28]          13,824\n",
      "        Identity-128          [-1, 288, 28, 28]               0\n",
      "            SiLU-129          [-1, 288, 28, 28]               0\n",
      "  BatchNormAct2d-130          [-1, 288, 28, 28]             576\n",
      "          Conv2d-131          [-1, 288, 28, 28]           7,200\n",
      "        Identity-132          [-1, 288, 28, 28]               0\n",
      "            SiLU-133          [-1, 288, 28, 28]               0\n",
      "  BatchNormAct2d-134          [-1, 288, 28, 28]             576\n",
      "          Conv2d-135             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-136             [-1, 12, 1, 1]               0\n",
      "          Conv2d-137            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-138            [-1, 288, 1, 1]               0\n",
      "   SqueezeExcite-139          [-1, 288, 28, 28]               0\n",
      "          Conv2d-140           [-1, 48, 28, 28]          13,824\n",
      "        Identity-141           [-1, 48, 28, 28]               0\n",
      "        Identity-142           [-1, 48, 28, 28]               0\n",
      "  BatchNormAct2d-143           [-1, 48, 28, 28]              96\n",
      "        Identity-144           [-1, 48, 28, 28]               0\n",
      "InvertedResidual-145           [-1, 48, 28, 28]               0\n",
      "          Conv2d-146          [-1, 288, 28, 28]          13,824\n",
      "        Identity-147          [-1, 288, 28, 28]               0\n",
      "            SiLU-148          [-1, 288, 28, 28]               0\n",
      "  BatchNormAct2d-149          [-1, 288, 28, 28]             576\n",
      "      Conv2dSame-150          [-1, 288, 14, 14]           2,592\n",
      "        Identity-151          [-1, 288, 14, 14]               0\n",
      "            SiLU-152          [-1, 288, 14, 14]               0\n",
      "  BatchNormAct2d-153          [-1, 288, 14, 14]             576\n",
      "          Conv2d-154             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-155             [-1, 12, 1, 1]               0\n",
      "          Conv2d-156            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-157            [-1, 288, 1, 1]               0\n",
      "   SqueezeExcite-158          [-1, 288, 14, 14]               0\n",
      "          Conv2d-159           [-1, 88, 14, 14]          25,344\n",
      "        Identity-160           [-1, 88, 14, 14]               0\n",
      "        Identity-161           [-1, 88, 14, 14]               0\n",
      "  BatchNormAct2d-162           [-1, 88, 14, 14]             176\n",
      "InvertedResidual-163           [-1, 88, 14, 14]               0\n",
      "          Conv2d-164          [-1, 528, 14, 14]          46,464\n",
      "        Identity-165          [-1, 528, 14, 14]               0\n",
      "            SiLU-166          [-1, 528, 14, 14]               0\n",
      "  BatchNormAct2d-167          [-1, 528, 14, 14]           1,056\n",
      "          Conv2d-168          [-1, 528, 14, 14]           4,752\n",
      "        Identity-169          [-1, 528, 14, 14]               0\n",
      "            SiLU-170          [-1, 528, 14, 14]               0\n",
      "  BatchNormAct2d-171          [-1, 528, 14, 14]           1,056\n",
      "          Conv2d-172             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-173             [-1, 22, 1, 1]               0\n",
      "          Conv2d-174            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-175            [-1, 528, 1, 1]               0\n",
      "   SqueezeExcite-176          [-1, 528, 14, 14]               0\n",
      "          Conv2d-177           [-1, 88, 14, 14]          46,464\n",
      "        Identity-178           [-1, 88, 14, 14]               0\n",
      "        Identity-179           [-1, 88, 14, 14]               0\n",
      "  BatchNormAct2d-180           [-1, 88, 14, 14]             176\n",
      "        Identity-181           [-1, 88, 14, 14]               0\n",
      "InvertedResidual-182           [-1, 88, 14, 14]               0\n",
      "          Conv2d-183          [-1, 528, 14, 14]          46,464\n",
      "        Identity-184          [-1, 528, 14, 14]               0\n",
      "            SiLU-185          [-1, 528, 14, 14]               0\n",
      "  BatchNormAct2d-186          [-1, 528, 14, 14]           1,056\n",
      "          Conv2d-187          [-1, 528, 14, 14]           4,752\n",
      "        Identity-188          [-1, 528, 14, 14]               0\n",
      "            SiLU-189          [-1, 528, 14, 14]               0\n",
      "  BatchNormAct2d-190          [-1, 528, 14, 14]           1,056\n",
      "          Conv2d-191             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-192             [-1, 22, 1, 1]               0\n",
      "          Conv2d-193            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-194            [-1, 528, 1, 1]               0\n",
      "   SqueezeExcite-195          [-1, 528, 14, 14]               0\n",
      "          Conv2d-196           [-1, 88, 14, 14]          46,464\n",
      "        Identity-197           [-1, 88, 14, 14]               0\n",
      "        Identity-198           [-1, 88, 14, 14]               0\n",
      "  BatchNormAct2d-199           [-1, 88, 14, 14]             176\n",
      "        Identity-200           [-1, 88, 14, 14]               0\n",
      "InvertedResidual-201           [-1, 88, 14, 14]               0\n",
      "          Conv2d-202          [-1, 528, 14, 14]          46,464\n",
      "        Identity-203          [-1, 528, 14, 14]               0\n",
      "            SiLU-204          [-1, 528, 14, 14]               0\n",
      "  BatchNormAct2d-205          [-1, 528, 14, 14]           1,056\n",
      "          Conv2d-206          [-1, 528, 14, 14]           4,752\n",
      "        Identity-207          [-1, 528, 14, 14]               0\n",
      "            SiLU-208          [-1, 528, 14, 14]               0\n",
      "  BatchNormAct2d-209          [-1, 528, 14, 14]           1,056\n",
      "          Conv2d-210             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-211             [-1, 22, 1, 1]               0\n",
      "          Conv2d-212            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-213            [-1, 528, 1, 1]               0\n",
      "   SqueezeExcite-214          [-1, 528, 14, 14]               0\n",
      "          Conv2d-215           [-1, 88, 14, 14]          46,464\n",
      "        Identity-216           [-1, 88, 14, 14]               0\n",
      "        Identity-217           [-1, 88, 14, 14]               0\n",
      "  BatchNormAct2d-218           [-1, 88, 14, 14]             176\n",
      "        Identity-219           [-1, 88, 14, 14]               0\n",
      "InvertedResidual-220           [-1, 88, 14, 14]               0\n",
      "          Conv2d-221          [-1, 528, 14, 14]          46,464\n",
      "        Identity-222          [-1, 528, 14, 14]               0\n",
      "            SiLU-223          [-1, 528, 14, 14]               0\n",
      "  BatchNormAct2d-224          [-1, 528, 14, 14]           1,056\n",
      "          Conv2d-225          [-1, 528, 14, 14]          13,200\n",
      "        Identity-226          [-1, 528, 14, 14]               0\n",
      "            SiLU-227          [-1, 528, 14, 14]               0\n",
      "  BatchNormAct2d-228          [-1, 528, 14, 14]           1,056\n",
      "          Conv2d-229             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-230             [-1, 22, 1, 1]               0\n",
      "          Conv2d-231            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-232            [-1, 528, 1, 1]               0\n",
      "   SqueezeExcite-233          [-1, 528, 14, 14]               0\n",
      "          Conv2d-234          [-1, 120, 14, 14]          63,360\n",
      "        Identity-235          [-1, 120, 14, 14]               0\n",
      "        Identity-236          [-1, 120, 14, 14]               0\n",
      "  BatchNormAct2d-237          [-1, 120, 14, 14]             240\n",
      "InvertedResidual-238          [-1, 120, 14, 14]               0\n",
      "          Conv2d-239          [-1, 720, 14, 14]          86,400\n",
      "        Identity-240          [-1, 720, 14, 14]               0\n",
      "            SiLU-241          [-1, 720, 14, 14]               0\n",
      "  BatchNormAct2d-242          [-1, 720, 14, 14]           1,440\n",
      "          Conv2d-243          [-1, 720, 14, 14]          18,000\n",
      "        Identity-244          [-1, 720, 14, 14]               0\n",
      "            SiLU-245          [-1, 720, 14, 14]               0\n",
      "  BatchNormAct2d-246          [-1, 720, 14, 14]           1,440\n",
      "          Conv2d-247             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-248             [-1, 30, 1, 1]               0\n",
      "          Conv2d-249            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-250            [-1, 720, 1, 1]               0\n",
      "   SqueezeExcite-251          [-1, 720, 14, 14]               0\n",
      "          Conv2d-252          [-1, 120, 14, 14]          86,400\n",
      "        Identity-253          [-1, 120, 14, 14]               0\n",
      "        Identity-254          [-1, 120, 14, 14]               0\n",
      "  BatchNormAct2d-255          [-1, 120, 14, 14]             240\n",
      "        Identity-256          [-1, 120, 14, 14]               0\n",
      "InvertedResidual-257          [-1, 120, 14, 14]               0\n",
      "          Conv2d-258          [-1, 720, 14, 14]          86,400\n",
      "        Identity-259          [-1, 720, 14, 14]               0\n",
      "            SiLU-260          [-1, 720, 14, 14]               0\n",
      "  BatchNormAct2d-261          [-1, 720, 14, 14]           1,440\n",
      "          Conv2d-262          [-1, 720, 14, 14]          18,000\n",
      "        Identity-263          [-1, 720, 14, 14]               0\n",
      "            SiLU-264          [-1, 720, 14, 14]               0\n",
      "  BatchNormAct2d-265          [-1, 720, 14, 14]           1,440\n",
      "          Conv2d-266             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-267             [-1, 30, 1, 1]               0\n",
      "          Conv2d-268            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-269            [-1, 720, 1, 1]               0\n",
      "   SqueezeExcite-270          [-1, 720, 14, 14]               0\n",
      "          Conv2d-271          [-1, 120, 14, 14]          86,400\n",
      "        Identity-272          [-1, 120, 14, 14]               0\n",
      "        Identity-273          [-1, 120, 14, 14]               0\n",
      "  BatchNormAct2d-274          [-1, 120, 14, 14]             240\n",
      "        Identity-275          [-1, 120, 14, 14]               0\n",
      "InvertedResidual-276          [-1, 120, 14, 14]               0\n",
      "          Conv2d-277          [-1, 720, 14, 14]          86,400\n",
      "        Identity-278          [-1, 720, 14, 14]               0\n",
      "            SiLU-279          [-1, 720, 14, 14]               0\n",
      "  BatchNormAct2d-280          [-1, 720, 14, 14]           1,440\n",
      "          Conv2d-281          [-1, 720, 14, 14]          18,000\n",
      "        Identity-282          [-1, 720, 14, 14]               0\n",
      "            SiLU-283          [-1, 720, 14, 14]               0\n",
      "  BatchNormAct2d-284          [-1, 720, 14, 14]           1,440\n",
      "          Conv2d-285             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-286             [-1, 30, 1, 1]               0\n",
      "          Conv2d-287            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-288            [-1, 720, 1, 1]               0\n",
      "   SqueezeExcite-289          [-1, 720, 14, 14]               0\n",
      "          Conv2d-290          [-1, 120, 14, 14]          86,400\n",
      "        Identity-291          [-1, 120, 14, 14]               0\n",
      "        Identity-292          [-1, 120, 14, 14]               0\n",
      "  BatchNormAct2d-293          [-1, 120, 14, 14]             240\n",
      "        Identity-294          [-1, 120, 14, 14]               0\n",
      "InvertedResidual-295          [-1, 120, 14, 14]               0\n",
      "          Conv2d-296          [-1, 720, 14, 14]          86,400\n",
      "        Identity-297          [-1, 720, 14, 14]               0\n",
      "            SiLU-298          [-1, 720, 14, 14]               0\n",
      "  BatchNormAct2d-299          [-1, 720, 14, 14]           1,440\n",
      "      Conv2dSame-300            [-1, 720, 7, 7]          18,000\n",
      "        Identity-301            [-1, 720, 7, 7]               0\n",
      "            SiLU-302            [-1, 720, 7, 7]               0\n",
      "  BatchNormAct2d-303            [-1, 720, 7, 7]           1,440\n",
      "          Conv2d-304             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-305             [-1, 30, 1, 1]               0\n",
      "          Conv2d-306            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-307            [-1, 720, 1, 1]               0\n",
      "   SqueezeExcite-308            [-1, 720, 7, 7]               0\n",
      "          Conv2d-309            [-1, 208, 7, 7]         149,760\n",
      "        Identity-310            [-1, 208, 7, 7]               0\n",
      "        Identity-311            [-1, 208, 7, 7]               0\n",
      "  BatchNormAct2d-312            [-1, 208, 7, 7]             416\n",
      "InvertedResidual-313            [-1, 208, 7, 7]               0\n",
      "          Conv2d-314           [-1, 1248, 7, 7]         259,584\n",
      "        Identity-315           [-1, 1248, 7, 7]               0\n",
      "            SiLU-316           [-1, 1248, 7, 7]               0\n",
      "  BatchNormAct2d-317           [-1, 1248, 7, 7]           2,496\n",
      "          Conv2d-318           [-1, 1248, 7, 7]          31,200\n",
      "        Identity-319           [-1, 1248, 7, 7]               0\n",
      "            SiLU-320           [-1, 1248, 7, 7]               0\n",
      "  BatchNormAct2d-321           [-1, 1248, 7, 7]           2,496\n",
      "          Conv2d-322             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-323             [-1, 52, 1, 1]               0\n",
      "          Conv2d-324           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-325           [-1, 1248, 1, 1]               0\n",
      "   SqueezeExcite-326           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-327            [-1, 208, 7, 7]         259,584\n",
      "        Identity-328            [-1, 208, 7, 7]               0\n",
      "        Identity-329            [-1, 208, 7, 7]               0\n",
      "  BatchNormAct2d-330            [-1, 208, 7, 7]             416\n",
      "        Identity-331            [-1, 208, 7, 7]               0\n",
      "InvertedResidual-332            [-1, 208, 7, 7]               0\n",
      "          Conv2d-333           [-1, 1248, 7, 7]         259,584\n",
      "        Identity-334           [-1, 1248, 7, 7]               0\n",
      "            SiLU-335           [-1, 1248, 7, 7]               0\n",
      "  BatchNormAct2d-336           [-1, 1248, 7, 7]           2,496\n",
      "          Conv2d-337           [-1, 1248, 7, 7]          31,200\n",
      "        Identity-338           [-1, 1248, 7, 7]               0\n",
      "            SiLU-339           [-1, 1248, 7, 7]               0\n",
      "  BatchNormAct2d-340           [-1, 1248, 7, 7]           2,496\n",
      "          Conv2d-341             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-342             [-1, 52, 1, 1]               0\n",
      "          Conv2d-343           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-344           [-1, 1248, 1, 1]               0\n",
      "   SqueezeExcite-345           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-346            [-1, 208, 7, 7]         259,584\n",
      "        Identity-347            [-1, 208, 7, 7]               0\n",
      "        Identity-348            [-1, 208, 7, 7]               0\n",
      "  BatchNormAct2d-349            [-1, 208, 7, 7]             416\n",
      "        Identity-350            [-1, 208, 7, 7]               0\n",
      "InvertedResidual-351            [-1, 208, 7, 7]               0\n",
      "          Conv2d-352           [-1, 1248, 7, 7]         259,584\n",
      "        Identity-353           [-1, 1248, 7, 7]               0\n",
      "            SiLU-354           [-1, 1248, 7, 7]               0\n",
      "  BatchNormAct2d-355           [-1, 1248, 7, 7]           2,496\n",
      "          Conv2d-356           [-1, 1248, 7, 7]          31,200\n",
      "        Identity-357           [-1, 1248, 7, 7]               0\n",
      "            SiLU-358           [-1, 1248, 7, 7]               0\n",
      "  BatchNormAct2d-359           [-1, 1248, 7, 7]           2,496\n",
      "          Conv2d-360             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-361             [-1, 52, 1, 1]               0\n",
      "          Conv2d-362           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-363           [-1, 1248, 1, 1]               0\n",
      "   SqueezeExcite-364           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-365            [-1, 208, 7, 7]         259,584\n",
      "        Identity-366            [-1, 208, 7, 7]               0\n",
      "        Identity-367            [-1, 208, 7, 7]               0\n",
      "  BatchNormAct2d-368            [-1, 208, 7, 7]             416\n",
      "        Identity-369            [-1, 208, 7, 7]               0\n",
      "InvertedResidual-370            [-1, 208, 7, 7]               0\n",
      "          Conv2d-371           [-1, 1248, 7, 7]         259,584\n",
      "        Identity-372           [-1, 1248, 7, 7]               0\n",
      "            SiLU-373           [-1, 1248, 7, 7]               0\n",
      "  BatchNormAct2d-374           [-1, 1248, 7, 7]           2,496\n",
      "          Conv2d-375           [-1, 1248, 7, 7]          31,200\n",
      "        Identity-376           [-1, 1248, 7, 7]               0\n",
      "            SiLU-377           [-1, 1248, 7, 7]               0\n",
      "  BatchNormAct2d-378           [-1, 1248, 7, 7]           2,496\n",
      "          Conv2d-379             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-380             [-1, 52, 1, 1]               0\n",
      "          Conv2d-381           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-382           [-1, 1248, 1, 1]               0\n",
      "   SqueezeExcite-383           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-384            [-1, 208, 7, 7]         259,584\n",
      "        Identity-385            [-1, 208, 7, 7]               0\n",
      "        Identity-386            [-1, 208, 7, 7]               0\n",
      "  BatchNormAct2d-387            [-1, 208, 7, 7]             416\n",
      "        Identity-388            [-1, 208, 7, 7]               0\n",
      "InvertedResidual-389            [-1, 208, 7, 7]               0\n",
      "          Conv2d-390           [-1, 1248, 7, 7]         259,584\n",
      "        Identity-391           [-1, 1248, 7, 7]               0\n",
      "            SiLU-392           [-1, 1248, 7, 7]               0\n",
      "  BatchNormAct2d-393           [-1, 1248, 7, 7]           2,496\n",
      "          Conv2d-394           [-1, 1248, 7, 7]          11,232\n",
      "        Identity-395           [-1, 1248, 7, 7]               0\n",
      "            SiLU-396           [-1, 1248, 7, 7]               0\n",
      "  BatchNormAct2d-397           [-1, 1248, 7, 7]           2,496\n",
      "          Conv2d-398             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-399             [-1, 52, 1, 1]               0\n",
      "          Conv2d-400           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-401           [-1, 1248, 1, 1]               0\n",
      "   SqueezeExcite-402           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-403            [-1, 352, 7, 7]         439,296\n",
      "        Identity-404            [-1, 352, 7, 7]               0\n",
      "        Identity-405            [-1, 352, 7, 7]               0\n",
      "  BatchNormAct2d-406            [-1, 352, 7, 7]             704\n",
      "InvertedResidual-407            [-1, 352, 7, 7]               0\n",
      "          Conv2d-408           [-1, 2112, 7, 7]         743,424\n",
      "        Identity-409           [-1, 2112, 7, 7]               0\n",
      "            SiLU-410           [-1, 2112, 7, 7]               0\n",
      "  BatchNormAct2d-411           [-1, 2112, 7, 7]           4,224\n",
      "          Conv2d-412           [-1, 2112, 7, 7]          19,008\n",
      "        Identity-413           [-1, 2112, 7, 7]               0\n",
      "            SiLU-414           [-1, 2112, 7, 7]               0\n",
      "  BatchNormAct2d-415           [-1, 2112, 7, 7]           4,224\n",
      "          Conv2d-416             [-1, 88, 1, 1]         185,944\n",
      "            SiLU-417             [-1, 88, 1, 1]               0\n",
      "          Conv2d-418           [-1, 2112, 1, 1]         187,968\n",
      "         Sigmoid-419           [-1, 2112, 1, 1]               0\n",
      "   SqueezeExcite-420           [-1, 2112, 7, 7]               0\n",
      "          Conv2d-421            [-1, 352, 7, 7]         743,424\n",
      "        Identity-422            [-1, 352, 7, 7]               0\n",
      "        Identity-423            [-1, 352, 7, 7]               0\n",
      "  BatchNormAct2d-424            [-1, 352, 7, 7]             704\n",
      "        Identity-425            [-1, 352, 7, 7]               0\n",
      "InvertedResidual-426            [-1, 352, 7, 7]               0\n",
      "          Conv2d-427           [-1, 1408, 7, 7]         495,616\n",
      "        Identity-428           [-1, 1408, 7, 7]               0\n",
      "            SiLU-429           [-1, 1408, 7, 7]               0\n",
      "  BatchNormAct2d-430           [-1, 1408, 7, 7]           2,816\n",
      "AdaptiveAvgPool2d-431           [-1, 1408, 1, 1]               0\n",
      "         Flatten-432                 [-1, 1408]               0\n",
      "SelectAdaptivePool2d-433                 [-1, 1408]               0\n",
      "          Linear-434                    [-1, 8]          11,272\n",
      "================================================================\n",
      "Total params: 7,712,266\n",
      "Trainable params: 7,712,266\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 339.57\n",
      "Params size (MB): 29.42\n",
      "Estimated Total Size (MB): 369.57\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(summary(model, (3, 224, 224))) # Summary of the model with input size (3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights obtained from: https://github.com/av-savchenko/face-emotion-recognition/blob/main/models/affectnet_emotions/enet_b2_8_best.pt\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "        Conv2dSame-1         [-1, 32, 130, 130]             864\n",
      "          Identity-2         [-1, 32, 130, 130]               0\n",
      "              SiLU-3         [-1, 32, 130, 130]               0\n",
      "    BatchNormAct2d-4         [-1, 32, 130, 130]              64\n",
      "            Conv2d-5         [-1, 32, 130, 130]             288\n",
      "          Identity-6         [-1, 32, 130, 130]               0\n",
      "              SiLU-7         [-1, 32, 130, 130]               0\n",
      "    BatchNormAct2d-8         [-1, 32, 130, 130]              64\n",
      "            Conv2d-9              [-1, 8, 1, 1]             264\n",
      "             SiLU-10              [-1, 8, 1, 1]               0\n",
      "           Conv2d-11             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-12             [-1, 32, 1, 1]               0\n",
      "    SqueezeExcite-13         [-1, 32, 130, 130]               0\n",
      "           Conv2d-14         [-1, 16, 130, 130]             512\n",
      "         Identity-15         [-1, 16, 130, 130]               0\n",
      "         Identity-16         [-1, 16, 130, 130]               0\n",
      "   BatchNormAct2d-17         [-1, 16, 130, 130]              32\n",
      "DepthwiseSeparableConv-18         [-1, 16, 130, 130]               0\n",
      "           Conv2d-19         [-1, 16, 130, 130]             144\n",
      "         Identity-20         [-1, 16, 130, 130]               0\n",
      "             SiLU-21         [-1, 16, 130, 130]               0\n",
      "   BatchNormAct2d-22         [-1, 16, 130, 130]              32\n",
      "           Conv2d-23              [-1, 4, 1, 1]              68\n",
      "             SiLU-24              [-1, 4, 1, 1]               0\n",
      "           Conv2d-25             [-1, 16, 1, 1]              80\n",
      "          Sigmoid-26             [-1, 16, 1, 1]               0\n",
      "    SqueezeExcite-27         [-1, 16, 130, 130]               0\n",
      "           Conv2d-28         [-1, 16, 130, 130]             256\n",
      "         Identity-29         [-1, 16, 130, 130]               0\n",
      "         Identity-30         [-1, 16, 130, 130]               0\n",
      "   BatchNormAct2d-31         [-1, 16, 130, 130]              32\n",
      "         Identity-32         [-1, 16, 130, 130]               0\n",
      "DepthwiseSeparableConv-33         [-1, 16, 130, 130]               0\n",
      "           Conv2d-34         [-1, 96, 130, 130]           1,536\n",
      "         Identity-35         [-1, 96, 130, 130]               0\n",
      "             SiLU-36         [-1, 96, 130, 130]               0\n",
      "   BatchNormAct2d-37         [-1, 96, 130, 130]             192\n",
      "       Conv2dSame-38           [-1, 96, 65, 65]             864\n",
      "         Identity-39           [-1, 96, 65, 65]               0\n",
      "             SiLU-40           [-1, 96, 65, 65]               0\n",
      "   BatchNormAct2d-41           [-1, 96, 65, 65]             192\n",
      "           Conv2d-42              [-1, 4, 1, 1]             388\n",
      "             SiLU-43              [-1, 4, 1, 1]               0\n",
      "           Conv2d-44             [-1, 96, 1, 1]             480\n",
      "          Sigmoid-45             [-1, 96, 1, 1]               0\n",
      "    SqueezeExcite-46           [-1, 96, 65, 65]               0\n",
      "           Conv2d-47           [-1, 24, 65, 65]           2,304\n",
      "         Identity-48           [-1, 24, 65, 65]               0\n",
      "         Identity-49           [-1, 24, 65, 65]               0\n",
      "   BatchNormAct2d-50           [-1, 24, 65, 65]              48\n",
      " InvertedResidual-51           [-1, 24, 65, 65]               0\n",
      "           Conv2d-52          [-1, 144, 65, 65]           3,456\n",
      "         Identity-53          [-1, 144, 65, 65]               0\n",
      "             SiLU-54          [-1, 144, 65, 65]               0\n",
      "   BatchNormAct2d-55          [-1, 144, 65, 65]             288\n",
      "           Conv2d-56          [-1, 144, 65, 65]           1,296\n",
      "         Identity-57          [-1, 144, 65, 65]               0\n",
      "             SiLU-58          [-1, 144, 65, 65]               0\n",
      "   BatchNormAct2d-59          [-1, 144, 65, 65]             288\n",
      "           Conv2d-60              [-1, 6, 1, 1]             870\n",
      "             SiLU-61              [-1, 6, 1, 1]               0\n",
      "           Conv2d-62            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-63            [-1, 144, 1, 1]               0\n",
      "    SqueezeExcite-64          [-1, 144, 65, 65]               0\n",
      "           Conv2d-65           [-1, 24, 65, 65]           3,456\n",
      "         Identity-66           [-1, 24, 65, 65]               0\n",
      "         Identity-67           [-1, 24, 65, 65]               0\n",
      "   BatchNormAct2d-68           [-1, 24, 65, 65]              48\n",
      "         Identity-69           [-1, 24, 65, 65]               0\n",
      " InvertedResidual-70           [-1, 24, 65, 65]               0\n",
      "           Conv2d-71          [-1, 144, 65, 65]           3,456\n",
      "         Identity-72          [-1, 144, 65, 65]               0\n",
      "             SiLU-73          [-1, 144, 65, 65]               0\n",
      "   BatchNormAct2d-74          [-1, 144, 65, 65]             288\n",
      "           Conv2d-75          [-1, 144, 65, 65]           1,296\n",
      "         Identity-76          [-1, 144, 65, 65]               0\n",
      "             SiLU-77          [-1, 144, 65, 65]               0\n",
      "   BatchNormAct2d-78          [-1, 144, 65, 65]             288\n",
      "           Conv2d-79              [-1, 6, 1, 1]             870\n",
      "             SiLU-80              [-1, 6, 1, 1]               0\n",
      "           Conv2d-81            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-82            [-1, 144, 1, 1]               0\n",
      "    SqueezeExcite-83          [-1, 144, 65, 65]               0\n",
      "           Conv2d-84           [-1, 24, 65, 65]           3,456\n",
      "         Identity-85           [-1, 24, 65, 65]               0\n",
      "         Identity-86           [-1, 24, 65, 65]               0\n",
      "   BatchNormAct2d-87           [-1, 24, 65, 65]              48\n",
      "         Identity-88           [-1, 24, 65, 65]               0\n",
      " InvertedResidual-89           [-1, 24, 65, 65]               0\n",
      "           Conv2d-90          [-1, 144, 65, 65]           3,456\n",
      "         Identity-91          [-1, 144, 65, 65]               0\n",
      "             SiLU-92          [-1, 144, 65, 65]               0\n",
      "   BatchNormAct2d-93          [-1, 144, 65, 65]             288\n",
      "       Conv2dSame-94          [-1, 144, 33, 33]           3,600\n",
      "         Identity-95          [-1, 144, 33, 33]               0\n",
      "             SiLU-96          [-1, 144, 33, 33]               0\n",
      "   BatchNormAct2d-97          [-1, 144, 33, 33]             288\n",
      "           Conv2d-98              [-1, 6, 1, 1]             870\n",
      "             SiLU-99              [-1, 6, 1, 1]               0\n",
      "          Conv2d-100            [-1, 144, 1, 1]           1,008\n",
      "         Sigmoid-101            [-1, 144, 1, 1]               0\n",
      "   SqueezeExcite-102          [-1, 144, 33, 33]               0\n",
      "          Conv2d-103           [-1, 48, 33, 33]           6,912\n",
      "        Identity-104           [-1, 48, 33, 33]               0\n",
      "        Identity-105           [-1, 48, 33, 33]               0\n",
      "  BatchNormAct2d-106           [-1, 48, 33, 33]              96\n",
      "InvertedResidual-107           [-1, 48, 33, 33]               0\n",
      "          Conv2d-108          [-1, 288, 33, 33]          13,824\n",
      "        Identity-109          [-1, 288, 33, 33]               0\n",
      "            SiLU-110          [-1, 288, 33, 33]               0\n",
      "  BatchNormAct2d-111          [-1, 288, 33, 33]             576\n",
      "          Conv2d-112          [-1, 288, 33, 33]           7,200\n",
      "        Identity-113          [-1, 288, 33, 33]               0\n",
      "            SiLU-114          [-1, 288, 33, 33]               0\n",
      "  BatchNormAct2d-115          [-1, 288, 33, 33]             576\n",
      "          Conv2d-116             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-117             [-1, 12, 1, 1]               0\n",
      "          Conv2d-118            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-119            [-1, 288, 1, 1]               0\n",
      "   SqueezeExcite-120          [-1, 288, 33, 33]               0\n",
      "          Conv2d-121           [-1, 48, 33, 33]          13,824\n",
      "        Identity-122           [-1, 48, 33, 33]               0\n",
      "        Identity-123           [-1, 48, 33, 33]               0\n",
      "  BatchNormAct2d-124           [-1, 48, 33, 33]              96\n",
      "        Identity-125           [-1, 48, 33, 33]               0\n",
      "InvertedResidual-126           [-1, 48, 33, 33]               0\n",
      "          Conv2d-127          [-1, 288, 33, 33]          13,824\n",
      "        Identity-128          [-1, 288, 33, 33]               0\n",
      "            SiLU-129          [-1, 288, 33, 33]               0\n",
      "  BatchNormAct2d-130          [-1, 288, 33, 33]             576\n",
      "          Conv2d-131          [-1, 288, 33, 33]           7,200\n",
      "        Identity-132          [-1, 288, 33, 33]               0\n",
      "            SiLU-133          [-1, 288, 33, 33]               0\n",
      "  BatchNormAct2d-134          [-1, 288, 33, 33]             576\n",
      "          Conv2d-135             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-136             [-1, 12, 1, 1]               0\n",
      "          Conv2d-137            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-138            [-1, 288, 1, 1]               0\n",
      "   SqueezeExcite-139          [-1, 288, 33, 33]               0\n",
      "          Conv2d-140           [-1, 48, 33, 33]          13,824\n",
      "        Identity-141           [-1, 48, 33, 33]               0\n",
      "        Identity-142           [-1, 48, 33, 33]               0\n",
      "  BatchNormAct2d-143           [-1, 48, 33, 33]              96\n",
      "        Identity-144           [-1, 48, 33, 33]               0\n",
      "InvertedResidual-145           [-1, 48, 33, 33]               0\n",
      "          Conv2d-146          [-1, 288, 33, 33]          13,824\n",
      "        Identity-147          [-1, 288, 33, 33]               0\n",
      "            SiLU-148          [-1, 288, 33, 33]               0\n",
      "  BatchNormAct2d-149          [-1, 288, 33, 33]             576\n",
      "      Conv2dSame-150          [-1, 288, 17, 17]           2,592\n",
      "        Identity-151          [-1, 288, 17, 17]               0\n",
      "            SiLU-152          [-1, 288, 17, 17]               0\n",
      "  BatchNormAct2d-153          [-1, 288, 17, 17]             576\n",
      "          Conv2d-154             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-155             [-1, 12, 1, 1]               0\n",
      "          Conv2d-156            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-157            [-1, 288, 1, 1]               0\n",
      "   SqueezeExcite-158          [-1, 288, 17, 17]               0\n",
      "          Conv2d-159           [-1, 88, 17, 17]          25,344\n",
      "        Identity-160           [-1, 88, 17, 17]               0\n",
      "        Identity-161           [-1, 88, 17, 17]               0\n",
      "  BatchNormAct2d-162           [-1, 88, 17, 17]             176\n",
      "InvertedResidual-163           [-1, 88, 17, 17]               0\n",
      "          Conv2d-164          [-1, 528, 17, 17]          46,464\n",
      "        Identity-165          [-1, 528, 17, 17]               0\n",
      "            SiLU-166          [-1, 528, 17, 17]               0\n",
      "  BatchNormAct2d-167          [-1, 528, 17, 17]           1,056\n",
      "          Conv2d-168          [-1, 528, 17, 17]           4,752\n",
      "        Identity-169          [-1, 528, 17, 17]               0\n",
      "            SiLU-170          [-1, 528, 17, 17]               0\n",
      "  BatchNormAct2d-171          [-1, 528, 17, 17]           1,056\n",
      "          Conv2d-172             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-173             [-1, 22, 1, 1]               0\n",
      "          Conv2d-174            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-175            [-1, 528, 1, 1]               0\n",
      "   SqueezeExcite-176          [-1, 528, 17, 17]               0\n",
      "          Conv2d-177           [-1, 88, 17, 17]          46,464\n",
      "        Identity-178           [-1, 88, 17, 17]               0\n",
      "        Identity-179           [-1, 88, 17, 17]               0\n",
      "  BatchNormAct2d-180           [-1, 88, 17, 17]             176\n",
      "        Identity-181           [-1, 88, 17, 17]               0\n",
      "InvertedResidual-182           [-1, 88, 17, 17]               0\n",
      "          Conv2d-183          [-1, 528, 17, 17]          46,464\n",
      "        Identity-184          [-1, 528, 17, 17]               0\n",
      "            SiLU-185          [-1, 528, 17, 17]               0\n",
      "  BatchNormAct2d-186          [-1, 528, 17, 17]           1,056\n",
      "          Conv2d-187          [-1, 528, 17, 17]           4,752\n",
      "        Identity-188          [-1, 528, 17, 17]               0\n",
      "            SiLU-189          [-1, 528, 17, 17]               0\n",
      "  BatchNormAct2d-190          [-1, 528, 17, 17]           1,056\n",
      "          Conv2d-191             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-192             [-1, 22, 1, 1]               0\n",
      "          Conv2d-193            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-194            [-1, 528, 1, 1]               0\n",
      "   SqueezeExcite-195          [-1, 528, 17, 17]               0\n",
      "          Conv2d-196           [-1, 88, 17, 17]          46,464\n",
      "        Identity-197           [-1, 88, 17, 17]               0\n",
      "        Identity-198           [-1, 88, 17, 17]               0\n",
      "  BatchNormAct2d-199           [-1, 88, 17, 17]             176\n",
      "        Identity-200           [-1, 88, 17, 17]               0\n",
      "InvertedResidual-201           [-1, 88, 17, 17]               0\n",
      "          Conv2d-202          [-1, 528, 17, 17]          46,464\n",
      "        Identity-203          [-1, 528, 17, 17]               0\n",
      "            SiLU-204          [-1, 528, 17, 17]               0\n",
      "  BatchNormAct2d-205          [-1, 528, 17, 17]           1,056\n",
      "          Conv2d-206          [-1, 528, 17, 17]           4,752\n",
      "        Identity-207          [-1, 528, 17, 17]               0\n",
      "            SiLU-208          [-1, 528, 17, 17]               0\n",
      "  BatchNormAct2d-209          [-1, 528, 17, 17]           1,056\n",
      "          Conv2d-210             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-211             [-1, 22, 1, 1]               0\n",
      "          Conv2d-212            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-213            [-1, 528, 1, 1]               0\n",
      "   SqueezeExcite-214          [-1, 528, 17, 17]               0\n",
      "          Conv2d-215           [-1, 88, 17, 17]          46,464\n",
      "        Identity-216           [-1, 88, 17, 17]               0\n",
      "        Identity-217           [-1, 88, 17, 17]               0\n",
      "  BatchNormAct2d-218           [-1, 88, 17, 17]             176\n",
      "        Identity-219           [-1, 88, 17, 17]               0\n",
      "InvertedResidual-220           [-1, 88, 17, 17]               0\n",
      "          Conv2d-221          [-1, 528, 17, 17]          46,464\n",
      "        Identity-222          [-1, 528, 17, 17]               0\n",
      "            SiLU-223          [-1, 528, 17, 17]               0\n",
      "  BatchNormAct2d-224          [-1, 528, 17, 17]           1,056\n",
      "          Conv2d-225          [-1, 528, 17, 17]          13,200\n",
      "        Identity-226          [-1, 528, 17, 17]               0\n",
      "            SiLU-227          [-1, 528, 17, 17]               0\n",
      "  BatchNormAct2d-228          [-1, 528, 17, 17]           1,056\n",
      "          Conv2d-229             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-230             [-1, 22, 1, 1]               0\n",
      "          Conv2d-231            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-232            [-1, 528, 1, 1]               0\n",
      "   SqueezeExcite-233          [-1, 528, 17, 17]               0\n",
      "          Conv2d-234          [-1, 120, 17, 17]          63,360\n",
      "        Identity-235          [-1, 120, 17, 17]               0\n",
      "        Identity-236          [-1, 120, 17, 17]               0\n",
      "  BatchNormAct2d-237          [-1, 120, 17, 17]             240\n",
      "InvertedResidual-238          [-1, 120, 17, 17]               0\n",
      "          Conv2d-239          [-1, 720, 17, 17]          86,400\n",
      "        Identity-240          [-1, 720, 17, 17]               0\n",
      "            SiLU-241          [-1, 720, 17, 17]               0\n",
      "  BatchNormAct2d-242          [-1, 720, 17, 17]           1,440\n",
      "          Conv2d-243          [-1, 720, 17, 17]          18,000\n",
      "        Identity-244          [-1, 720, 17, 17]               0\n",
      "            SiLU-245          [-1, 720, 17, 17]               0\n",
      "  BatchNormAct2d-246          [-1, 720, 17, 17]           1,440\n",
      "          Conv2d-247             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-248             [-1, 30, 1, 1]               0\n",
      "          Conv2d-249            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-250            [-1, 720, 1, 1]               0\n",
      "   SqueezeExcite-251          [-1, 720, 17, 17]               0\n",
      "          Conv2d-252          [-1, 120, 17, 17]          86,400\n",
      "        Identity-253          [-1, 120, 17, 17]               0\n",
      "        Identity-254          [-1, 120, 17, 17]               0\n",
      "  BatchNormAct2d-255          [-1, 120, 17, 17]             240\n",
      "        Identity-256          [-1, 120, 17, 17]               0\n",
      "InvertedResidual-257          [-1, 120, 17, 17]               0\n",
      "          Conv2d-258          [-1, 720, 17, 17]          86,400\n",
      "        Identity-259          [-1, 720, 17, 17]               0\n",
      "            SiLU-260          [-1, 720, 17, 17]               0\n",
      "  BatchNormAct2d-261          [-1, 720, 17, 17]           1,440\n",
      "          Conv2d-262          [-1, 720, 17, 17]          18,000\n",
      "        Identity-263          [-1, 720, 17, 17]               0\n",
      "            SiLU-264          [-1, 720, 17, 17]               0\n",
      "  BatchNormAct2d-265          [-1, 720, 17, 17]           1,440\n",
      "          Conv2d-266             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-267             [-1, 30, 1, 1]               0\n",
      "          Conv2d-268            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-269            [-1, 720, 1, 1]               0\n",
      "   SqueezeExcite-270          [-1, 720, 17, 17]               0\n",
      "          Conv2d-271          [-1, 120, 17, 17]          86,400\n",
      "        Identity-272          [-1, 120, 17, 17]               0\n",
      "        Identity-273          [-1, 120, 17, 17]               0\n",
      "  BatchNormAct2d-274          [-1, 120, 17, 17]             240\n",
      "        Identity-275          [-1, 120, 17, 17]               0\n",
      "InvertedResidual-276          [-1, 120, 17, 17]               0\n",
      "          Conv2d-277          [-1, 720, 17, 17]          86,400\n",
      "        Identity-278          [-1, 720, 17, 17]               0\n",
      "            SiLU-279          [-1, 720, 17, 17]               0\n",
      "  BatchNormAct2d-280          [-1, 720, 17, 17]           1,440\n",
      "          Conv2d-281          [-1, 720, 17, 17]          18,000\n",
      "        Identity-282          [-1, 720, 17, 17]               0\n",
      "            SiLU-283          [-1, 720, 17, 17]               0\n",
      "  BatchNormAct2d-284          [-1, 720, 17, 17]           1,440\n",
      "          Conv2d-285             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-286             [-1, 30, 1, 1]               0\n",
      "          Conv2d-287            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-288            [-1, 720, 1, 1]               0\n",
      "   SqueezeExcite-289          [-1, 720, 17, 17]               0\n",
      "          Conv2d-290          [-1, 120, 17, 17]          86,400\n",
      "        Identity-291          [-1, 120, 17, 17]               0\n",
      "        Identity-292          [-1, 120, 17, 17]               0\n",
      "  BatchNormAct2d-293          [-1, 120, 17, 17]             240\n",
      "        Identity-294          [-1, 120, 17, 17]               0\n",
      "InvertedResidual-295          [-1, 120, 17, 17]               0\n",
      "          Conv2d-296          [-1, 720, 17, 17]          86,400\n",
      "        Identity-297          [-1, 720, 17, 17]               0\n",
      "            SiLU-298          [-1, 720, 17, 17]               0\n",
      "  BatchNormAct2d-299          [-1, 720, 17, 17]           1,440\n",
      "      Conv2dSame-300            [-1, 720, 9, 9]          18,000\n",
      "        Identity-301            [-1, 720, 9, 9]               0\n",
      "            SiLU-302            [-1, 720, 9, 9]               0\n",
      "  BatchNormAct2d-303            [-1, 720, 9, 9]           1,440\n",
      "          Conv2d-304             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-305             [-1, 30, 1, 1]               0\n",
      "          Conv2d-306            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-307            [-1, 720, 1, 1]               0\n",
      "   SqueezeExcite-308            [-1, 720, 9, 9]               0\n",
      "          Conv2d-309            [-1, 208, 9, 9]         149,760\n",
      "        Identity-310            [-1, 208, 9, 9]               0\n",
      "        Identity-311            [-1, 208, 9, 9]               0\n",
      "  BatchNormAct2d-312            [-1, 208, 9, 9]             416\n",
      "InvertedResidual-313            [-1, 208, 9, 9]               0\n",
      "          Conv2d-314           [-1, 1248, 9, 9]         259,584\n",
      "        Identity-315           [-1, 1248, 9, 9]               0\n",
      "            SiLU-316           [-1, 1248, 9, 9]               0\n",
      "  BatchNormAct2d-317           [-1, 1248, 9, 9]           2,496\n",
      "          Conv2d-318           [-1, 1248, 9, 9]          31,200\n",
      "        Identity-319           [-1, 1248, 9, 9]               0\n",
      "            SiLU-320           [-1, 1248, 9, 9]               0\n",
      "  BatchNormAct2d-321           [-1, 1248, 9, 9]           2,496\n",
      "          Conv2d-322             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-323             [-1, 52, 1, 1]               0\n",
      "          Conv2d-324           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-325           [-1, 1248, 1, 1]               0\n",
      "   SqueezeExcite-326           [-1, 1248, 9, 9]               0\n",
      "          Conv2d-327            [-1, 208, 9, 9]         259,584\n",
      "        Identity-328            [-1, 208, 9, 9]               0\n",
      "        Identity-329            [-1, 208, 9, 9]               0\n",
      "  BatchNormAct2d-330            [-1, 208, 9, 9]             416\n",
      "        Identity-331            [-1, 208, 9, 9]               0\n",
      "InvertedResidual-332            [-1, 208, 9, 9]               0\n",
      "          Conv2d-333           [-1, 1248, 9, 9]         259,584\n",
      "        Identity-334           [-1, 1248, 9, 9]               0\n",
      "            SiLU-335           [-1, 1248, 9, 9]               0\n",
      "  BatchNormAct2d-336           [-1, 1248, 9, 9]           2,496\n",
      "          Conv2d-337           [-1, 1248, 9, 9]          31,200\n",
      "        Identity-338           [-1, 1248, 9, 9]               0\n",
      "            SiLU-339           [-1, 1248, 9, 9]               0\n",
      "  BatchNormAct2d-340           [-1, 1248, 9, 9]           2,496\n",
      "          Conv2d-341             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-342             [-1, 52, 1, 1]               0\n",
      "          Conv2d-343           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-344           [-1, 1248, 1, 1]               0\n",
      "   SqueezeExcite-345           [-1, 1248, 9, 9]               0\n",
      "          Conv2d-346            [-1, 208, 9, 9]         259,584\n",
      "        Identity-347            [-1, 208, 9, 9]               0\n",
      "        Identity-348            [-1, 208, 9, 9]               0\n",
      "  BatchNormAct2d-349            [-1, 208, 9, 9]             416\n",
      "        Identity-350            [-1, 208, 9, 9]               0\n",
      "InvertedResidual-351            [-1, 208, 9, 9]               0\n",
      "          Conv2d-352           [-1, 1248, 9, 9]         259,584\n",
      "        Identity-353           [-1, 1248, 9, 9]               0\n",
      "            SiLU-354           [-1, 1248, 9, 9]               0\n",
      "  BatchNormAct2d-355           [-1, 1248, 9, 9]           2,496\n",
      "          Conv2d-356           [-1, 1248, 9, 9]          31,200\n",
      "        Identity-357           [-1, 1248, 9, 9]               0\n",
      "            SiLU-358           [-1, 1248, 9, 9]               0\n",
      "  BatchNormAct2d-359           [-1, 1248, 9, 9]           2,496\n",
      "          Conv2d-360             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-361             [-1, 52, 1, 1]               0\n",
      "          Conv2d-362           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-363           [-1, 1248, 1, 1]               0\n",
      "   SqueezeExcite-364           [-1, 1248, 9, 9]               0\n",
      "          Conv2d-365            [-1, 208, 9, 9]         259,584\n",
      "        Identity-366            [-1, 208, 9, 9]               0\n",
      "        Identity-367            [-1, 208, 9, 9]               0\n",
      "  BatchNormAct2d-368            [-1, 208, 9, 9]             416\n",
      "        Identity-369            [-1, 208, 9, 9]               0\n",
      "InvertedResidual-370            [-1, 208, 9, 9]               0\n",
      "          Conv2d-371           [-1, 1248, 9, 9]         259,584\n",
      "        Identity-372           [-1, 1248, 9, 9]               0\n",
      "            SiLU-373           [-1, 1248, 9, 9]               0\n",
      "  BatchNormAct2d-374           [-1, 1248, 9, 9]           2,496\n",
      "          Conv2d-375           [-1, 1248, 9, 9]          31,200\n",
      "        Identity-376           [-1, 1248, 9, 9]               0\n",
      "            SiLU-377           [-1, 1248, 9, 9]               0\n",
      "  BatchNormAct2d-378           [-1, 1248, 9, 9]           2,496\n",
      "          Conv2d-379             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-380             [-1, 52, 1, 1]               0\n",
      "          Conv2d-381           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-382           [-1, 1248, 1, 1]               0\n",
      "   SqueezeExcite-383           [-1, 1248, 9, 9]               0\n",
      "          Conv2d-384            [-1, 208, 9, 9]         259,584\n",
      "        Identity-385            [-1, 208, 9, 9]               0\n",
      "        Identity-386            [-1, 208, 9, 9]               0\n",
      "  BatchNormAct2d-387            [-1, 208, 9, 9]             416\n",
      "        Identity-388            [-1, 208, 9, 9]               0\n",
      "InvertedResidual-389            [-1, 208, 9, 9]               0\n",
      "          Conv2d-390           [-1, 1248, 9, 9]         259,584\n",
      "        Identity-391           [-1, 1248, 9, 9]               0\n",
      "            SiLU-392           [-1, 1248, 9, 9]               0\n",
      "  BatchNormAct2d-393           [-1, 1248, 9, 9]           2,496\n",
      "          Conv2d-394           [-1, 1248, 9, 9]          11,232\n",
      "        Identity-395           [-1, 1248, 9, 9]               0\n",
      "            SiLU-396           [-1, 1248, 9, 9]               0\n",
      "  BatchNormAct2d-397           [-1, 1248, 9, 9]           2,496\n",
      "          Conv2d-398             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-399             [-1, 52, 1, 1]               0\n",
      "          Conv2d-400           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-401           [-1, 1248, 1, 1]               0\n",
      "   SqueezeExcite-402           [-1, 1248, 9, 9]               0\n",
      "          Conv2d-403            [-1, 352, 9, 9]         439,296\n",
      "        Identity-404            [-1, 352, 9, 9]               0\n",
      "        Identity-405            [-1, 352, 9, 9]               0\n",
      "  BatchNormAct2d-406            [-1, 352, 9, 9]             704\n",
      "InvertedResidual-407            [-1, 352, 9, 9]               0\n",
      "          Conv2d-408           [-1, 2112, 9, 9]         743,424\n",
      "        Identity-409           [-1, 2112, 9, 9]               0\n",
      "            SiLU-410           [-1, 2112, 9, 9]               0\n",
      "  BatchNormAct2d-411           [-1, 2112, 9, 9]           4,224\n",
      "          Conv2d-412           [-1, 2112, 9, 9]          19,008\n",
      "        Identity-413           [-1, 2112, 9, 9]               0\n",
      "            SiLU-414           [-1, 2112, 9, 9]               0\n",
      "  BatchNormAct2d-415           [-1, 2112, 9, 9]           4,224\n",
      "          Conv2d-416             [-1, 88, 1, 1]         185,944\n",
      "            SiLU-417             [-1, 88, 1, 1]               0\n",
      "          Conv2d-418           [-1, 2112, 1, 1]         187,968\n",
      "         Sigmoid-419           [-1, 2112, 1, 1]               0\n",
      "   SqueezeExcite-420           [-1, 2112, 9, 9]               0\n",
      "          Conv2d-421            [-1, 352, 9, 9]         743,424\n",
      "        Identity-422            [-1, 352, 9, 9]               0\n",
      "        Identity-423            [-1, 352, 9, 9]               0\n",
      "  BatchNormAct2d-424            [-1, 352, 9, 9]             704\n",
      "        Identity-425            [-1, 352, 9, 9]               0\n",
      "InvertedResidual-426            [-1, 352, 9, 9]               0\n",
      "          Conv2d-427           [-1, 1408, 9, 9]         495,616\n",
      "        Identity-428           [-1, 1408, 9, 9]               0\n",
      "            SiLU-429           [-1, 1408, 9, 9]               0\n",
      "  BatchNormAct2d-430           [-1, 1408, 9, 9]           2,816\n",
      "AdaptiveMaxPool2d-431           [-1, 1408, 1, 1]               0\n",
      "         Flatten-432                 [-1, 1408]               0\n",
      "SelectAdaptivePool2d-433                 [-1, 1408]               0\n",
      "          Linear-434                    [-1, 8]          11,272\n",
      "================================================================\n",
      "Total params: 7,712,266\n",
      "Trainable params: 7,712,266\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.77\n",
      "Forward/backward pass size (MB): 479.19\n",
      "Params size (MB): 29.42\n",
      "Estimated Total Size (MB): 509.39\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "weights_path = os.path.join(MODELS_DIR,\"EfficientNetB2\", \"enet_b2_8_best.pt\")\n",
    "print(\"Weights obtained from: https://github.com/av-savchenko/face-emotion-recognition/blob/main/models/affectnet_emotions/enet_b2_8_best.pt\")\n",
    "model = torch.load(weights_path)\n",
    "print(summary(model, (3, 260, 260))) # Summary of the model with input size (3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA with 1 GPUs\n",
      "Using CUDA device:NVIDIA GeForce GTX 1080 Ti\n",
      "Weights obtained from: https://github.com/av-savchenko/face-emotion-recognition/blob/main/models/affectnet_emotions/enet_b2_8_best.pt\n",
      "Linear(in_features=1408, out_features=8, bias=True)\n"
     ]
    }
   ],
   "source": [
    "efficientnet, device = arch.model_creation(arch_type = \"efficientnet_b2\", weights = \"affectnet_cat_emot\")\n",
    "print(model.classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeiT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA with 1 GPUs\n",
      "Using CUDA device:NVIDIA GeForce GTX 1080 Ti\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e039fec798a14b67ac539db170216ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/23.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "torch.Size([96, 8])\n",
      "torch.Size([96, 8])\n",
      "Epoch 1:\n",
      "torch.Size([96, 8])\n",
      "torch.Size([96, 8])\n",
      "Epoch 2:\n",
      "torch.Size([96, 8])\n",
      "torch.Size([96, 8])\n",
      "Epoch 3:\n",
      "torch.Size([96, 8])\n",
      "torch.Size([96, 8])\n",
      "Epoch 4:\n",
      "torch.Size([96, 8])\n",
      "torch.Size([96, 8])\n",
      "Epoch 5:\n",
      "torch.Size([96, 8])\n",
      "torch.Size([96, 8])\n",
      "Epoch 6:\n",
      "torch.Size([96, 8])\n",
      "torch.Size([96, 8])\n",
      "Epoch 7:\n",
      "torch.Size([96, 8])\n",
      "torch.Size([96, 8])\n",
      "Epoch 8:\n",
      "torch.Size([96, 8])\n",
      "torch.Size([96, 8])\n",
      "Epoch 9:\n",
      "torch.Size([96, 8])\n",
      "torch.Size([96, 8])\n"
     ]
    }
   ],
   "source": [
    "model, device = arch.model_creation(arch_type=\"deit_tiny\", weights=\"imagenet\")\n",
    "\n",
    "batch_size = 96\n",
    "for epoch in range(10):\n",
    "    # Run the model\n",
    "    input_tensor = torch.rand(batch_size, 3, 224, 224).to(device)  # Batch size 32\n",
    "    pred, dist_pred = model(input_tensor)\n",
    "    print(\"Epoch \", epoch, \":\", sep =\"\")\n",
    "    print(pred.size())\n",
    "    print(dist_pred.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 192, 14, 14]         147,648\n",
      "          Identity-2             [-1, 196, 192]               0\n",
      "        PatchEmbed-3             [-1, 196, 192]               0\n",
      "           Dropout-4             [-1, 198, 192]               0\n",
      "          Identity-5             [-1, 198, 192]               0\n",
      "          Identity-6             [-1, 198, 192]               0\n",
      "         LayerNorm-7             [-1, 198, 192]             384\n",
      "            Linear-8             [-1, 198, 576]         111,168\n",
      "          Identity-9           [-1, 3, 198, 64]               0\n",
      "         Identity-10           [-1, 3, 198, 64]               0\n",
      "           Linear-11             [-1, 198, 192]          37,056\n",
      "          Dropout-12             [-1, 198, 192]               0\n",
      "        Attention-13             [-1, 198, 192]               0\n",
      "         Identity-14             [-1, 198, 192]               0\n",
      "         Identity-15             [-1, 198, 192]               0\n",
      "        LayerNorm-16             [-1, 198, 192]             384\n",
      "           Linear-17             [-1, 198, 768]         148,224\n",
      "             GELU-18             [-1, 198, 768]               0\n",
      "          Dropout-19             [-1, 198, 768]               0\n",
      "         Identity-20             [-1, 198, 768]               0\n",
      "           Linear-21             [-1, 198, 192]         147,648\n",
      "          Dropout-22             [-1, 198, 192]               0\n",
      "              Mlp-23             [-1, 198, 192]               0\n",
      "         Identity-24             [-1, 198, 192]               0\n",
      "         Identity-25             [-1, 198, 192]               0\n",
      "            Block-26             [-1, 198, 192]               0\n",
      "        LayerNorm-27             [-1, 198, 192]             384\n",
      "           Linear-28             [-1, 198, 576]         111,168\n",
      "         Identity-29           [-1, 3, 198, 64]               0\n",
      "         Identity-30           [-1, 3, 198, 64]               0\n",
      "           Linear-31             [-1, 198, 192]          37,056\n",
      "          Dropout-32             [-1, 198, 192]               0\n",
      "        Attention-33             [-1, 198, 192]               0\n",
      "         Identity-34             [-1, 198, 192]               0\n",
      "         Identity-35             [-1, 198, 192]               0\n",
      "        LayerNorm-36             [-1, 198, 192]             384\n",
      "           Linear-37             [-1, 198, 768]         148,224\n",
      "             GELU-38             [-1, 198, 768]               0\n",
      "          Dropout-39             [-1, 198, 768]               0\n",
      "         Identity-40             [-1, 198, 768]               0\n",
      "           Linear-41             [-1, 198, 192]         147,648\n",
      "          Dropout-42             [-1, 198, 192]               0\n",
      "              Mlp-43             [-1, 198, 192]               0\n",
      "         Identity-44             [-1, 198, 192]               0\n",
      "         Identity-45             [-1, 198, 192]               0\n",
      "            Block-46             [-1, 198, 192]               0\n",
      "        LayerNorm-47             [-1, 198, 192]             384\n",
      "           Linear-48             [-1, 198, 576]         111,168\n",
      "         Identity-49           [-1, 3, 198, 64]               0\n",
      "         Identity-50           [-1, 3, 198, 64]               0\n",
      "           Linear-51             [-1, 198, 192]          37,056\n",
      "          Dropout-52             [-1, 198, 192]               0\n",
      "        Attention-53             [-1, 198, 192]               0\n",
      "         Identity-54             [-1, 198, 192]               0\n",
      "         Identity-55             [-1, 198, 192]               0\n",
      "        LayerNorm-56             [-1, 198, 192]             384\n",
      "           Linear-57             [-1, 198, 768]         148,224\n",
      "             GELU-58             [-1, 198, 768]               0\n",
      "          Dropout-59             [-1, 198, 768]               0\n",
      "         Identity-60             [-1, 198, 768]               0\n",
      "           Linear-61             [-1, 198, 192]         147,648\n",
      "          Dropout-62             [-1, 198, 192]               0\n",
      "              Mlp-63             [-1, 198, 192]               0\n",
      "         Identity-64             [-1, 198, 192]               0\n",
      "         Identity-65             [-1, 198, 192]               0\n",
      "            Block-66             [-1, 198, 192]               0\n",
      "        LayerNorm-67             [-1, 198, 192]             384\n",
      "           Linear-68             [-1, 198, 576]         111,168\n",
      "         Identity-69           [-1, 3, 198, 64]               0\n",
      "         Identity-70           [-1, 3, 198, 64]               0\n",
      "           Linear-71             [-1, 198, 192]          37,056\n",
      "          Dropout-72             [-1, 198, 192]               0\n",
      "        Attention-73             [-1, 198, 192]               0\n",
      "         Identity-74             [-1, 198, 192]               0\n",
      "         Identity-75             [-1, 198, 192]               0\n",
      "        LayerNorm-76             [-1, 198, 192]             384\n",
      "           Linear-77             [-1, 198, 768]         148,224\n",
      "             GELU-78             [-1, 198, 768]               0\n",
      "          Dropout-79             [-1, 198, 768]               0\n",
      "         Identity-80             [-1, 198, 768]               0\n",
      "           Linear-81             [-1, 198, 192]         147,648\n",
      "          Dropout-82             [-1, 198, 192]               0\n",
      "              Mlp-83             [-1, 198, 192]               0\n",
      "         Identity-84             [-1, 198, 192]               0\n",
      "         Identity-85             [-1, 198, 192]               0\n",
      "            Block-86             [-1, 198, 192]               0\n",
      "        LayerNorm-87             [-1, 198, 192]             384\n",
      "           Linear-88             [-1, 198, 576]         111,168\n",
      "         Identity-89           [-1, 3, 198, 64]               0\n",
      "         Identity-90           [-1, 3, 198, 64]               0\n",
      "           Linear-91             [-1, 198, 192]          37,056\n",
      "          Dropout-92             [-1, 198, 192]               0\n",
      "        Attention-93             [-1, 198, 192]               0\n",
      "         Identity-94             [-1, 198, 192]               0\n",
      "         Identity-95             [-1, 198, 192]               0\n",
      "        LayerNorm-96             [-1, 198, 192]             384\n",
      "           Linear-97             [-1, 198, 768]         148,224\n",
      "             GELU-98             [-1, 198, 768]               0\n",
      "          Dropout-99             [-1, 198, 768]               0\n",
      "        Identity-100             [-1, 198, 768]               0\n",
      "          Linear-101             [-1, 198, 192]         147,648\n",
      "         Dropout-102             [-1, 198, 192]               0\n",
      "             Mlp-103             [-1, 198, 192]               0\n",
      "        Identity-104             [-1, 198, 192]               0\n",
      "        Identity-105             [-1, 198, 192]               0\n",
      "           Block-106             [-1, 198, 192]               0\n",
      "       LayerNorm-107             [-1, 198, 192]             384\n",
      "          Linear-108             [-1, 198, 576]         111,168\n",
      "        Identity-109           [-1, 3, 198, 64]               0\n",
      "        Identity-110           [-1, 3, 198, 64]               0\n",
      "          Linear-111             [-1, 198, 192]          37,056\n",
      "         Dropout-112             [-1, 198, 192]               0\n",
      "       Attention-113             [-1, 198, 192]               0\n",
      "        Identity-114             [-1, 198, 192]               0\n",
      "        Identity-115             [-1, 198, 192]               0\n",
      "       LayerNorm-116             [-1, 198, 192]             384\n",
      "          Linear-117             [-1, 198, 768]         148,224\n",
      "            GELU-118             [-1, 198, 768]               0\n",
      "         Dropout-119             [-1, 198, 768]               0\n",
      "        Identity-120             [-1, 198, 768]               0\n",
      "          Linear-121             [-1, 198, 192]         147,648\n",
      "         Dropout-122             [-1, 198, 192]               0\n",
      "             Mlp-123             [-1, 198, 192]               0\n",
      "        Identity-124             [-1, 198, 192]               0\n",
      "        Identity-125             [-1, 198, 192]               0\n",
      "           Block-126             [-1, 198, 192]               0\n",
      "       LayerNorm-127             [-1, 198, 192]             384\n",
      "          Linear-128             [-1, 198, 576]         111,168\n",
      "        Identity-129           [-1, 3, 198, 64]               0\n",
      "        Identity-130           [-1, 3, 198, 64]               0\n",
      "          Linear-131             [-1, 198, 192]          37,056\n",
      "         Dropout-132             [-1, 198, 192]               0\n",
      "       Attention-133             [-1, 198, 192]               0\n",
      "        Identity-134             [-1, 198, 192]               0\n",
      "        Identity-135             [-1, 198, 192]               0\n",
      "       LayerNorm-136             [-1, 198, 192]             384\n",
      "          Linear-137             [-1, 198, 768]         148,224\n",
      "            GELU-138             [-1, 198, 768]               0\n",
      "         Dropout-139             [-1, 198, 768]               0\n",
      "        Identity-140             [-1, 198, 768]               0\n",
      "          Linear-141             [-1, 198, 192]         147,648\n",
      "         Dropout-142             [-1, 198, 192]               0\n",
      "             Mlp-143             [-1, 198, 192]               0\n",
      "        Identity-144             [-1, 198, 192]               0\n",
      "        Identity-145             [-1, 198, 192]               0\n",
      "           Block-146             [-1, 198, 192]               0\n",
      "       LayerNorm-147             [-1, 198, 192]             384\n",
      "          Linear-148             [-1, 198, 576]         111,168\n",
      "        Identity-149           [-1, 3, 198, 64]               0\n",
      "        Identity-150           [-1, 3, 198, 64]               0\n",
      "          Linear-151             [-1, 198, 192]          37,056\n",
      "         Dropout-152             [-1, 198, 192]               0\n",
      "       Attention-153             [-1, 198, 192]               0\n",
      "        Identity-154             [-1, 198, 192]               0\n",
      "        Identity-155             [-1, 198, 192]               0\n",
      "       LayerNorm-156             [-1, 198, 192]             384\n",
      "          Linear-157             [-1, 198, 768]         148,224\n",
      "            GELU-158             [-1, 198, 768]               0\n",
      "         Dropout-159             [-1, 198, 768]               0\n",
      "        Identity-160             [-1, 198, 768]               0\n",
      "          Linear-161             [-1, 198, 192]         147,648\n",
      "         Dropout-162             [-1, 198, 192]               0\n",
      "             Mlp-163             [-1, 198, 192]               0\n",
      "        Identity-164             [-1, 198, 192]               0\n",
      "        Identity-165             [-1, 198, 192]               0\n",
      "           Block-166             [-1, 198, 192]               0\n",
      "       LayerNorm-167             [-1, 198, 192]             384\n",
      "          Linear-168             [-1, 198, 576]         111,168\n",
      "        Identity-169           [-1, 3, 198, 64]               0\n",
      "        Identity-170           [-1, 3, 198, 64]               0\n",
      "          Linear-171             [-1, 198, 192]          37,056\n",
      "         Dropout-172             [-1, 198, 192]               0\n",
      "       Attention-173             [-1, 198, 192]               0\n",
      "        Identity-174             [-1, 198, 192]               0\n",
      "        Identity-175             [-1, 198, 192]               0\n",
      "       LayerNorm-176             [-1, 198, 192]             384\n",
      "          Linear-177             [-1, 198, 768]         148,224\n",
      "            GELU-178             [-1, 198, 768]               0\n",
      "         Dropout-179             [-1, 198, 768]               0\n",
      "        Identity-180             [-1, 198, 768]               0\n",
      "          Linear-181             [-1, 198, 192]         147,648\n",
      "         Dropout-182             [-1, 198, 192]               0\n",
      "             Mlp-183             [-1, 198, 192]               0\n",
      "        Identity-184             [-1, 198, 192]               0\n",
      "        Identity-185             [-1, 198, 192]               0\n",
      "           Block-186             [-1, 198, 192]               0\n",
      "       LayerNorm-187             [-1, 198, 192]             384\n",
      "          Linear-188             [-1, 198, 576]         111,168\n",
      "        Identity-189           [-1, 3, 198, 64]               0\n",
      "        Identity-190           [-1, 3, 198, 64]               0\n",
      "          Linear-191             [-1, 198, 192]          37,056\n",
      "         Dropout-192             [-1, 198, 192]               0\n",
      "       Attention-193             [-1, 198, 192]               0\n",
      "        Identity-194             [-1, 198, 192]               0\n",
      "        Identity-195             [-1, 198, 192]               0\n",
      "       LayerNorm-196             [-1, 198, 192]             384\n",
      "          Linear-197             [-1, 198, 768]         148,224\n",
      "            GELU-198             [-1, 198, 768]               0\n",
      "         Dropout-199             [-1, 198, 768]               0\n",
      "        Identity-200             [-1, 198, 768]               0\n",
      "          Linear-201             [-1, 198, 192]         147,648\n",
      "         Dropout-202             [-1, 198, 192]               0\n",
      "             Mlp-203             [-1, 198, 192]               0\n",
      "        Identity-204             [-1, 198, 192]               0\n",
      "        Identity-205             [-1, 198, 192]               0\n",
      "           Block-206             [-1, 198, 192]               0\n",
      "       LayerNorm-207             [-1, 198, 192]             384\n",
      "          Linear-208             [-1, 198, 576]         111,168\n",
      "        Identity-209           [-1, 3, 198, 64]               0\n",
      "        Identity-210           [-1, 3, 198, 64]               0\n",
      "          Linear-211             [-1, 198, 192]          37,056\n",
      "         Dropout-212             [-1, 198, 192]               0\n",
      "       Attention-213             [-1, 198, 192]               0\n",
      "        Identity-214             [-1, 198, 192]               0\n",
      "        Identity-215             [-1, 198, 192]               0\n",
      "       LayerNorm-216             [-1, 198, 192]             384\n",
      "          Linear-217             [-1, 198, 768]         148,224\n",
      "            GELU-218             [-1, 198, 768]               0\n",
      "         Dropout-219             [-1, 198, 768]               0\n",
      "        Identity-220             [-1, 198, 768]               0\n",
      "          Linear-221             [-1, 198, 192]         147,648\n",
      "         Dropout-222             [-1, 198, 192]               0\n",
      "             Mlp-223             [-1, 198, 192]               0\n",
      "        Identity-224             [-1, 198, 192]               0\n",
      "        Identity-225             [-1, 198, 192]               0\n",
      "           Block-226             [-1, 198, 192]               0\n",
      "       LayerNorm-227             [-1, 198, 192]             384\n",
      "          Linear-228             [-1, 198, 576]         111,168\n",
      "        Identity-229           [-1, 3, 198, 64]               0\n",
      "        Identity-230           [-1, 3, 198, 64]               0\n",
      "          Linear-231             [-1, 198, 192]          37,056\n",
      "         Dropout-232             [-1, 198, 192]               0\n",
      "       Attention-233             [-1, 198, 192]               0\n",
      "        Identity-234             [-1, 198, 192]               0\n",
      "        Identity-235             [-1, 198, 192]               0\n",
      "       LayerNorm-236             [-1, 198, 192]             384\n",
      "          Linear-237             [-1, 198, 768]         148,224\n",
      "            GELU-238             [-1, 198, 768]               0\n",
      "         Dropout-239             [-1, 198, 768]               0\n",
      "        Identity-240             [-1, 198, 768]               0\n",
      "          Linear-241             [-1, 198, 192]         147,648\n",
      "         Dropout-242             [-1, 198, 192]               0\n",
      "             Mlp-243             [-1, 198, 192]               0\n",
      "        Identity-244             [-1, 198, 192]               0\n",
      "        Identity-245             [-1, 198, 192]               0\n",
      "           Block-246             [-1, 198, 192]               0\n",
      "       LayerNorm-247             [-1, 198, 192]             384\n",
      "          Linear-248                 [-1, 1000]         193,000\n",
      "          Linear-249                 [-1, 1000]         193,000\n",
      "================================================================\n",
      "Total params: 5,872,400\n",
      "Trainable params: 5,872,400\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 120.37\n",
      "Params size (MB): 22.40\n",
      "Estimated Total Size (MB): 143.35\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "DeiT_model(\n",
      "  (base_model): VisionTransformerDistilled(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (patch_drop): Identity()\n",
      "    (norm_pre): Identity()\n",
      "    (blocks): Sequential(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (fc_norm): Identity()\n",
      "    (head_drop): Dropout(p=0.0, inplace=False)\n",
      "    (head): Linear(in_features=192, out_features=8, bias=True)\n",
      "    (head_dist): Linear(in_features=192, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_t = timm.create_model('timm/deit_tiny_distilled_patch16_224.fb_in1k', pretrained=True).to(device)\n",
    "model_s = timm.create_model('timm/deit_small_distilled_patch16_224.fb_in1k', pretrained=True).to(device)\n",
    "model_b = timm.create_model('timm/deit_base_distilled_patch16_224.fb_in1k', pretrained=True).to(device)\n",
    "print(summary(model_t, (3, 224, 224))) # Summary of the model with input size (3, 224, 224)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 198, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "tensor(0.6802, grad_fn=<SumBackward1>)\n",
      "0.68021625\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('deit_base_distilled_patch16_224.fb_in1k', pretrained=True, num_classes = 8)\n",
    "positional_embeddings = model.pos_embed\n",
    "\n",
    "print(positional_embeddings.size())\n",
    "dist_token = model.dist_token\n",
    "print(dist_token.size())\n",
    "cls_token = model.cls_token\n",
    "print(cls_token.size())\n",
    "cos_sim = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "cosine = cos_sim(cls_token.squeeze(),dist_token.squeeze())\n",
    "print(cosine)\n",
    "cls_token = cls_token.squeeze().detach().numpy()\n",
    "dist_token = dist_token.squeeze().detach().numpy()\n",
    "cosine = np.dot(cls_token,dist_token)/(norm(cls_token)*norm(dist_token))\n",
    "print(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: cls_token\n",
      "Size: torch.Size([1, 1, 768])\n",
      "Name: pos_embed\n",
      "Size: torch.Size([1, 198, 768])\n",
      "Name: dist_token\n",
      "Size: torch.Size([1, 1, 768])\n",
      "Name: patch_embed.proj.weight\n",
      "Size: torch.Size([768, 3, 16, 16])\n",
      "Name: patch_embed.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.0.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.0.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.0.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.0.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.0.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.0.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.0.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.0.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.0.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.0.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.0.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.0.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.1.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.1.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.1.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.1.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.1.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.1.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.1.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.1.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.1.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.1.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.1.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.1.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.2.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.2.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.2.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.2.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.2.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.2.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.2.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.2.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.2.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.2.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.2.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.2.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.3.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.3.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.3.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.3.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.3.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.3.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.3.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.3.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.3.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.3.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.3.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.3.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.4.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.4.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.4.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.4.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.4.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.4.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.4.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.4.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.4.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.4.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.4.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.4.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.5.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.5.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.5.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.5.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.5.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.5.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.5.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.5.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.5.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.5.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.5.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.5.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.6.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.6.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.6.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.6.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.6.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.6.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.6.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.6.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.6.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.6.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.6.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.6.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.7.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.7.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.7.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.7.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.7.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.7.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.7.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.7.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.7.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.7.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.7.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.7.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.8.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.8.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.8.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.8.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.8.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.8.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.8.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.8.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.8.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.8.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.8.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.8.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.9.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.9.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.9.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.9.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.9.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.9.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.9.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.9.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.9.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.9.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.9.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.9.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.10.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.10.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.10.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.10.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.10.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.10.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.10.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.10.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.10.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.10.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.10.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.10.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.11.norm1.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.11.norm1.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.11.attn.qkv.weight\n",
      "Size: torch.Size([2304, 768])\n",
      "Name: blocks.11.attn.qkv.bias\n",
      "Size: torch.Size([2304])\n",
      "Name: blocks.11.attn.proj.weight\n",
      "Size: torch.Size([768, 768])\n",
      "Name: blocks.11.attn.proj.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.11.norm2.weight\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.11.norm2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: blocks.11.mlp.fc1.weight\n",
      "Size: torch.Size([3072, 768])\n",
      "Name: blocks.11.mlp.fc1.bias\n",
      "Size: torch.Size([3072])\n",
      "Name: blocks.11.mlp.fc2.weight\n",
      "Size: torch.Size([768, 3072])\n",
      "Name: blocks.11.mlp.fc2.bias\n",
      "Size: torch.Size([768])\n",
      "Name: norm.weight\n",
      "Size: torch.Size([768])\n",
      "Name: norm.bias\n",
      "Size: torch.Size([768])\n",
      "Name: head.weight\n",
      "Size: torch.Size([8, 768])\n",
      "Name: head.bias\n",
      "Size: torch.Size([8])\n",
      "Name: head_dist.weight\n",
      "Size: torch.Size([8, 768])\n",
      "Name: head_dist.bias\n",
      "Size: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the model's parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print('Name:', name)\n",
    "    print('Size:', param.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
