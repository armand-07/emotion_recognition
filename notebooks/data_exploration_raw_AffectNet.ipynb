{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration on AffectNet raw dataset\n",
    "First we declare the necessary variables and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "try: \n",
    "    from src import RAW_AFFECTNET_DIR\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Ensure that src is added to PATH and restart the kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training path is: /mnt/gpid08/datasets/affectnet/raw/train_set\n",
      "The validation path is: /mnt/gpid08/datasets/affectnet/raw/val_set\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(RAW_AFFECTNET_DIR, \"train_set\")\n",
    "val_path = os.path.join(RAW_AFFECTNET_DIR, \"val_set\")\n",
    "print(\"The training path is:\", train_path)\n",
    "print(\"The validation path is:\", val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many annotations there are in the dataset adn how many photos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1150604 annotation files in the training set\n",
      "There are 15996 annotation files in the validation set\n",
      "There are 287651 images in the training set\n",
      "There are 3999 images in the validation set\n"
     ]
    }
   ],
   "source": [
    "train_path_annotation = os.path.join(train_path, \"annotations\")\n",
    "val_path_annotation = os.path.join(val_path, \"annotations\")\n",
    "train_path_images = os.path.join(train_path, \"images\")\n",
    "val_path_images = os.path.join(val_path, \"images\")\n",
    "\n",
    "print(\"There are\", len(os.listdir(train_path_annotation)), \"annotation files in the training set\")\n",
    "print(\"There are\", len(os.listdir(val_path_annotation)), \"annotation files in the validation set\")\n",
    "print(\"There are\", len(os.listdir(train_path_images)), \"images in the training set\")\n",
    "print(\"There are\", len(os.listdir(val_path_images)), \"images in the validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe they do not have the same number of annotations and photos. This is because per each photo there are many files representing different annotations. We will see which kind of annotations are there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check kind of annotations in the files and how many there are of each kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file types for train are: {'aro': 287651, 'exp': 287651, 'lnd': 287651, 'val': 287651}\n",
      "The file types for val are: {'val': 3999, 'exp': 3999, 'lnd': 3999, 'aro': 3999}\n"
     ]
    }
   ],
   "source": [
    "# Check the number of files in the training set\n",
    "\n",
    "file_type_set = dict()\n",
    "for file in os.listdir(train_path_annotation):\n",
    "    file_type = file.split(\"_\")[1].split(\".\")[0]\n",
    "    if file.endswith(\".npy\"):\n",
    "        if file_type not in file_type_set:\n",
    "            file_type_set[file_type] = 1\n",
    "        else:\n",
    "            file_type_set[file_type] += 1\n",
    "print(\"The file types for train are:\", file_type_set)\n",
    "\n",
    "# Check now the validation set\n",
    "val_path_annotation = os.path.join(val_path, \"annotations\")\n",
    "file_type_set = dict()\n",
    "for file in os.listdir(val_path_annotation):\n",
    "    file_type = file.split(\"_\")[1].split(\".\")[0]\n",
    "    if file.endswith(\".npy\"):\n",
    "        if file_type not in file_type_set:\n",
    "            file_type_set[file_type] = 1\n",
    "        else:\n",
    "            file_type_set[file_type] += 1\n",
    "print(\"The file types for val are:\", file_type_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that there are the same amount of annotations for each image. The type of annotations is the same one. Now we will check that the number of annotations per id is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 287651 id in the training set\n",
      "There are 3999 id in the validation set\n"
     ]
    }
   ],
   "source": [
    "# Check the number of files in the training set\n",
    "id_dict_train = dict()\n",
    "for file in os.listdir(train_path_annotation):\n",
    "    id = file.split(\"_\")[0]\n",
    "    if file.endswith(\".npy\"):\n",
    "        if id not in id_dict_train:\n",
    "            id_dict_train[id] = 1\n",
    "        else:\n",
    "            id_dict_train[id] += 1\n",
    "print(\"There are\", len(id_dict_train), \"id in the training set\")\n",
    "\n",
    "# Check now the validation set\n",
    "id_dict_val = dict()\n",
    "for file in os.listdir(val_path_annotation):\n",
    "    id = file.split(\"_\")[0]\n",
    "    if file.endswith(\".npy\"):\n",
    "        if id not in id_dict_val:\n",
    "            id_dict_val[id] = 1\n",
    "        else:\n",
    "            id_dict_val[id] += 1\n",
    "print(\"There are\", len(id_dict_val), \"id in the validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ids have 4 annotations in 'train' set\n",
      "All ids have 4 annotations in 'val' set\n"
     ]
    }
   ],
   "source": [
    "if any(value != 4 for value in id_dict_train.values()):\n",
    "    print(\"There is at least one id in 'train' set with less/more than 4 annotations\")\n",
    "else:\n",
    "    print(\"All ids have 4 annotations in 'train' set\")\n",
    "\n",
    "if any(value != 4 for value in id_dict_val.values()):\n",
    "    print(\"There is at least one id in 'val' set with less/more than 4 annotations\")\n",
    "else:\n",
    "    print(\"All ids have 4 annotations in 'val' set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check that per each annotation, the number of expected values inside and the data type is correct:\n",
    "\n",
    "* `val`: Valence. The expected length of numpy vector is 1. The data type is <Ux (variable length x unicode string encoded using little-endian). The data type represented is float. The accepted range is [-1,1].\n",
    "* `aro`: Arousal. The expected length of numpy vector is 1. The data type is <Ux (variable length x unicode string encoded using little-endian). The data type represented is float. The accepted range is [-1,1].\n",
    "* `exp`: Expression (categorical emotion). The expected length of numpy vector is 1. The data type is <Ux (variable length x unicode string encoded using little-endian). The data type represented is integer. The accepted values are [0, 1, 2, 3, 4, 5, 6, 7] from .\n",
    "* `lnd`: landmarks of the face. The expected length of numpy vector is 136 (68 points with succesive x and y coordinates). The data type is float for each coordinate.\n",
    ", and  are stored using unicode strings, for lnd is used the float type (but as I have not much interest in this landmarks i will not be using them).\n",
    "\n",
    "If no output is printed(excepting the helping function to analyze it), this means that the rules are followed.\n",
    "\n",
    "**Cited from the AffectNet8**\n",
    "\n",
    "* The x and y coordination and the annotations are stored in .npy files. Use Python numpy to read the files separated with a semi-colon. \n",
    "* Expression: expression ID of the face (0: Neutral, 1: Happy, 2: Sad, 3:Surprise, 4: Fear, 5: Disgust, 6: Anger, 7: Contempt) \n",
    "* Valence: valence value of the expression in interval [-1,+1] (for Uncertain and No-face categories the value is -2) \n",
    "* Arousal: arousal value of the expression in interval [-1,+1] (for Uncertain and No-face categories the value is -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the annotations in the training set\n",
      "Analyzed 0 ids from 287651\n",
      "Analyzed 25000 ids from 287651\n",
      "Analyzed 50000 ids from 287651\n",
      "Analyzed 75000 ids from 287651\n",
      "Analyzed 100000 ids from 287651\n",
      "Analyzed 125000 ids from 287651\n",
      "Analyzed 150000 ids from 287651\n",
      "Analyzed 175000 ids from 287651\n",
      "Analyzed 200000 ids from 287651\n",
      "Analyzed 225000 ids from 287651\n",
      "Analyzed 250000 ids from 287651\n",
      "Analyzed 275000 ids from 287651\n",
      "Analyzing the annotations in the validation set\n",
      "Analyzed 0 ids from 3999\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing the annotations in the training set\")\n",
    "analyzed_ids = 0\n",
    "for id in id_dict_train.keys():\n",
    "    for filename in file_type_set.keys():\n",
    "        data = np.load(os.path.join(train_path_annotation, id + \"_\" + filename + \".npy\"))\n",
    "        if filename == 'lnd':\n",
    "            if data.size != 136:\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has\", data.size, \"annotations from the standard 136\")\n",
    "            if not np.issubdtype(data.dtype, np.floating):\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has no float values\")\n",
    "        else:\n",
    "            if data.size != 1:\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has\", data.size, \"annotations\")\n",
    "            if not np.issubdtype(data.dtype, np.unicode_):\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has been stored using other type than string\")\n",
    "        \n",
    "        if filename == 'exp':\n",
    "            if int(data) not in [0, 1, 2, 3, 4, 5, 6, 7]: # Available expressions\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has an expression value of\", int(data))\n",
    "        elif filename == 'val' or filename == 'aro':\n",
    "            if float(data) < -1.0 or float(data) > 1.0:\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has a value of\", float(data), \"for\", filename)\n",
    "    if analyzed_ids % 25000 == 0:\n",
    "        print(\"Analyzed\", analyzed_ids, \"ids from\", len(id_dict_train))\n",
    "    analyzed_ids += 1\n",
    "\n",
    "print(\"Analyzing the annotations in the validation set\")\n",
    "analyzed_ids = 0\n",
    "for id in id_dict_val.keys():\n",
    "    for filename in file_type_set.keys():\n",
    "        data = np.load(os.path.join(val_path_annotation, id + \"_\" + filename + \".npy\"))\n",
    "        if filename == 'lnd':\n",
    "            if data.size != 136:\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has\", data.size, \"annotations from the standard 136\")\n",
    "            if not np.issubdtype(data.dtype, np.floating):\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has no float values\")\n",
    "        else:\n",
    "            if data.size != 1:\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has\", data.size, \"annotations\")\n",
    "                \n",
    "            if not np.issubdtype(data.dtype, np.unicode_):\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has been stored using other type than string\")\n",
    "\n",
    "        if filename == 'exp':\n",
    "            if int(data) not in [0, 1, 2, 3, 4, 5, 6, 7]: # Available expressions\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has an expression value of\", int(data))\n",
    "        elif filename == 'val' or filename == 'aro':\n",
    "            if float(data) < -1.0 or float(data) > 1.0:\n",
    "                print(\"The file\", id + \"_\" + filename + \".npy\", \"has a value of\", float(data), \"for\", filename)\n",
    "    if analyzed_ids % 10000 == 0:\n",
    "        print(\"Analyzed\", analyzed_ids, \"ids from\", len(id_dict_val))\n",
    "    analyzed_ids += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we check that per each annotation id known, it has their own image file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have an annotation on 'train' set, and all annotations have an image\n",
      "All images have an annotation on 'val' set, and all annotations have an image\n"
     ]
    }
   ],
   "source": [
    "# Convert the dictionary to a set\n",
    "id_set_train = set(id_dict_train)\n",
    "for image_path in os.listdir(train_path_images):\n",
    "    id = image_path.split(\".\")[0]\n",
    "    if id in id_set_train:\n",
    "        id_set_train.remove(id)\n",
    "    else :\n",
    "        print(\"The image\", image_path, \"is not present in the training set\")\n",
    "\n",
    "if (len(id_set_train) == 0):\n",
    "    print(\"All images have an annotation on 'train' set, and all annotations have an image\")\n",
    "\n",
    "id_set_val = set(id_dict_val)\n",
    "for image_path in os.listdir(val_path_images):\n",
    "    id = image_path.split(\".\")[0]\n",
    "    if id in id_set_val:\n",
    "        id_set_val.remove(id)\n",
    "    else :\n",
    "        print(\"The image\", image_path, \"is not present in the validation set\")\n",
    "\n",
    "if (len(id_set_val) == 0):\n",
    "    print(\"All images have an annotation on 'val' set, and all annotations have an image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We have checked that the dataset structure to see if we can do a fast and easy data loading for interim creation. We have seen that the dataset is well structured and we can load it easily. Also the lnd files are not necessary for our project, so we will not be using them (as we don't expect to have a landmark detection model in our project). Furthermore, we have confirmed that each photo has the same amount of annotations, and that each annotation has the same amount of values. We have also checked that the values are in the expected range and that the data type is correct. Finally, we have checked that each annotation id has their own image file.\n",
    "\n",
    "There are nly train and validation sets, so we will have to remake the partitions for data preprocessing.\n",
    "\n",
    "Steps to do in interim:\n",
    "* Check if data is well distributed and if we have to do some data augmentation.\n",
    "* Images can be loaded and not corrupt, and their size is 224x244.\n",
    "* The val/aro does not appear to show -2 values for no-face or uncertain (no value is lower than -2), so we will have to check if there are some images with this values.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
